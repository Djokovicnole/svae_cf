{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')\n",
    "    \n",
    "def plot_len_vs_ndcg(len_to_ndcg_at_100_map):\n",
    "    \n",
    "    X_mvae_netflix = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 192, 194, 195, 196, 197, 198, 199, 200]\n",
    "    Y_mvae_netflix = [17.305459805165263, 18.271700778729297, 19.939235397849227, 20.615807779071446, 21.200536304667036, 22.668206604989916, 23.62762111889102, 24.1113769160321, 24.38576784295414, 24.651906312317955, 24.572177126181142, 24.097612237466684, 23.361091530883265, 22.961639225853908, 22.380616601147683, 22.570365422661485, 23.226473729740466, 23.55808194301172, 23.905232990628896, 24.67609924695598, 24.790753393108893, 24.699341448600368, 24.329625071791032, 24.34869768801409, 24.255240658584107, 23.8571758262351, 23.59269525353753, 23.732532176874358, 24.30481045301082, 23.70875891299226, 23.81468415071772, 23.64207029682717, 24.173490233620566, 23.01290689462342, 23.44197538425504, 23.115913341499848, 23.312578258072, 22.424685145841657, 23.41878275174073, 22.859779958188703, 23.478619357990972, 23.722274237306653, 24.111670143968666, 23.103060793686897, 23.261865738449355, 22.530793995200533, 22.46222150614473, 22.412599304559407, 22.414945145593123, 22.31086851922349, 22.17761897571351, 21.88076868966187, 21.768170856351094, 22.028155064021156, 21.71684577025067, 21.22732928030378, 20.609413899994195, 19.800055402123387, 19.381503434943006, 19.603972896073685, 19.862933729279177, 19.799475300978543, 19.843200740090065, 19.8870036930784, 19.477139969242245, 19.4631147662293, 19.728978375382674, 20.224790558321782, 19.94221122018891, 20.21560702269877, 19.66444342853739, 19.676951322853924, 20.232758626818303, 20.184517727868005, 19.51926377235292, 19.835604528281827, 20.13285325658148, 19.852840347617573, 19.557859649630423, 19.732197250399338, 20.036022080266843, 18.835314931400724, 19.317207401473976, 19.647503418236795, 19.658537264648448, 18.29569173398534, 18.696987223207543, 17.679959148502046, 17.47765647083317, 17.084835433149657, 17.4203317776519, 17.83049612504641, 17.154211742812475, 16.502866586006053, 17.147364004991793, 16.752288905284654, 15.554803397344633, 15.490649701740205, 15.365954299879494, 15.234798613879189, 16.116028759933034, 16.777042712910383, 17.474257186260022, 18.662170625953102, 18.25708466548021, 18.044341072937918, 17.732739778385657, 17.70456456016834, 16.5946121967739, 16.643935340980068, 16.03153896942279, 16.17903722214664, 15.48849466167474, 16.334893836091368, 15.980542629478663, 16.223256166311366, 16.99047381857931, 18.494408413854845, 19.07966764732772, 18.69168202040712, 19.56019857225473, 19.55335660690402, 18.942472171512193, 17.848991098200255, 19.33229713629901, 19.644705606240155, 19.58588755723168, 20.371979264473254, 20.969026944425785, 19.618733831284523, 18.939347373823544, 18.2926115801619, 17.905964313216998, 17.513850083910732, 18.954914320807354, 19.516843305093158, 19.16239432825069, 18.04474632050065, 18.471264893817626, 17.280071669030267, 17.779933740252858, 18.17361340040471, 19.33458238658996, 19.704600520321375, 20.796094783953684, 20.32572057556, 21.25600269355972, 20.451352094565802, 20.46366934934908, 18.66755377700566, 17.285141822485848, 17.008886326565896, 17.116426241913263, 17.17191393661636, 17.740607538964838, 18.31040755631411, 16.941964737975486, 17.255713721885556, 15.545303497687973, 15.032243814270108, 15.67488045996087, 15.75790529362007, 15.821777605379534, 16.51116636728033, 17.8806208772918, 18.55545955039922, 20.051227883380886, 20.478703679872652, 20.60864595614596, 21.17214481181908, 19.670823745486132, 20.093266457401263, 17.656685144370634, 19.484065103835995, 19.01808282525615, 18.803368344628062, 19.347414884808465, 21.33056403749105, 19.916156599030852, 20.305107750441515, 21.87774985484024, 19.306331918087075, 19.73467045052776, 21.06087808019119, 20.203695908837613, 20.374355222523583, 23.96444695089085, 22.23914388628821, 19.475972613773443, 17.480550314956872, 16.66621357973195, 14.26344037396423, 14.024447491048988, 15.53859478055956, 19.81240003959899, 17.975738014329544, 19.355593730440038]\n",
    "\n",
    "    #X_caser_netflix = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 76, 78, 79, 82, 83, 84, 87, 88, 89, 93, 97, 99, 102, 103, 109, 110, 113, 121, 127, 136, 152, 160, 163]\n",
    "    #Y_caser_netflix = [16.485085672216037, 18.747429945674003, 20.70896325823069, 22.274748734876773, 23.958296584266115, 26.55151746407545, 29.031670288905907, 30.299196231145896, 31.853266701606675, 32.368682209107874, 34.47022374355164, 31.586889875434753, 31.312491843238597, 29.68294276103537, 28.738119281356415, 26.783961566150033, 28.240157730515865, 28.081689400389923, 27.02566266144924, 26.864874439062373, 26.491010579644854, 27.09857589397714, 26.19853959140324, 27.01040859620567, 26.466000049491253, 24.743734902944883, 23.97695258723686, 23.796191184057946, 24.092584600277316, 23.30043531446083, 25.78689753003254, 26.85863210242469, 27.87029236537054, 27.4475221523274, 26.869912639341567, 26.297849906130665, 23.302306922109842, 21.775402832671514, 20.068269790515046, 21.185051385338472, 20.967370945317732, 20.98113222440973, 21.307069353022985, 21.599862392432176, 20.42497393290995, 18.795313318889207, 19.740882624603966, 20.57957208190134, 24.21087122007957, 23.501963985112432, 24.89061382552697, 22.842888656903167, 23.72005618702017, 17.62799016897126, 19.85917928876337, 19.64989000883951, 24.727002309823725, 21.87704201967477, 24.724995559321986, 26.368554354877375, 25.783655106475372, 23.354241485490224, 22.33465457893313, 20.327298031223584, 15.1265894762262, 16.06087006844015, 16.11003479892775, 19.48637251559304, 22.620831283556992, 22.289844898040194, 20.747426916367914, 19.251491528058374, 17.494689186306818, 17.279438001137468, 18.88280796886336, 19.40042651825565, 23.031706931139745, 23.669073803533642, 23.18997694201091, 23.744868407686305, 24.122741837443613, 21.667662813739867, 18.900717516150163, 15.77712867987563, 17.92647075389864, 16.93509903749765, 16.651559515501425, 20.77953950073382, 21.493690688749748, 17.479524350881874, 22.400733446255664, 27.065161696330712, 25.216483919020387]\n",
    "    \n",
    "    X_mvae_ml1m = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 55, 57, 58, 60, 61, 63, 64, 66, 69, 70, 82, 84, 87, 89, 93, 99, 103, 113, 127, 163]\n",
    "    Y_mvae_ml1m = [17.458343004804494, 25.68476725993679, 25.720223215791837, 25.597031830677235, 25.60894056452916, 27.212572655779933, 25.64280500408476, 26.946060711011416, 26.63155525896347, 26.570093706839668, 26.25265966672373, 26.24333246048654, 25.53109395164712, 25.196273849545193, 24.887480921672573, 24.18543800818245, 22.92177142891318, 20.19508267535594, 19.332850481686087, 18.517308642420755, 18.342761260362618, 17.728572394294858, 18.537252938520773, 18.833241815050933, 19.17857499436072, 17.889340756422133, 18.65369315804576, 20.091951078456706, 20.75806352196072, 22.47769631516441, 22.675843256048417, 21.77756921701781, 20.79989233259795, 20.63145877709492, 18.5998405400912, 18.082530407190426, 17.894127770884484, 15.353203887559113, 15.952639014230744, 15.252710639204718, 16.385665444911506, 13.741593127699593, 15.00266567689466, 16.058235079070204, 19.37848315523579, 18.86218161381533, 22.846203388265362, 23.962431804326116, 22.00613586999038, 21.265969002801462, 22.787056310591787, 21.015072022172696, 20.750919652745413, 21.33487821640185, 22.696750014491947, 23.157508889235253, 23.489746031457884, 23.8165395345481, 21.325967130172135, 15.702065969721346, 13.880828878658292, 11.60894842735441, 10.700079545157555, 12.676074201036371, 14.020292706307837, 16.627015897770967, 21.424928736322777, 22.043991130616757, 21.148713304006623, 20.69858947097228, 16.900969363977584, 11.989916720631882]\n",
    "    \n",
    "    X_caser_ml1m = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 76, 78, 79, 82, 83, 84, 87, 88, 89, 93, 97, 99, 102, 103, 109, 110, 113, 121, 127, 136, 152, 160, 163]\n",
    "    Y_caser_ml1m = [0.0, 10.028344645594448, 14.675889953551462, 17.470951011175373, 20.028055079279955, 26.048766562683625, 28.985644886068997, 30.136517017997345, 31.869327633606026, 32.22796180526963, 34.40297811810012, 31.57751429369091, 31.96384441141034, 30.55161384867838, 29.564890409671175, 27.238779335887745, 28.46575997413736, 27.862309945175745, 26.67645852086431, 27.10916797648568, 27.395403915717544, 28.404598171346578, 27.973075729696006, 28.789272924188147, 28.272827832551002, 25.92681241747378, 24.778747620954505, 24.62396512082558, 24.838277801073225, 23.661084591940796, 25.838047310063935, 26.869790890333366, 27.597128392306296, 27.061480455510967, 26.755869573244354, 26.593943270171486, 23.021211306902835, 21.636638272898683, 19.936419004270302, 20.538356426127613, 21.04889999521382, 21.206476475116077, 20.474327432544616, 20.730500913675513, 19.93946359899678, 17.712291313211857, 18.64911430487431, 20.298932327380292, 24.403281324875632, 23.433749785945583, 25.368204238274927, 23.37669250346416, 25.80389470323312, 19.19739761774908, 21.46835758531339, 20.384420617398426, 26.334306692738778, 22.09894233910496, 25.359592992933283, 26.130830435162466, 25.585404139918975, 22.95491894754602, 21.055773604095247, 19.51537657965822, 14.927087080211985, 15.168698621408788, 14.383734424676339, 18.093075400458435, 20.309942526601898, 20.049415003003276, 18.916706848132385, 17.73412995496482, 15.478143185394885, 15.462911347213828, 17.06556583817946, 16.276450064533826, 20.011038854979766, 21.92519228005839, 21.93205044224887, 22.963563328098783, 25.069187771767922, 22.597460974218386, 19.63699555209318, 16.49247702383932, 19.26356429088387, 18.962639345958138, 18.792142931066223, 22.94660734039426, 24.315044451797572, 18.649558650859287, 23.00657864264672, 28.49204066525045, 27.134879344897275]\n",
    "    \n",
    "    lens = list(len_to_ndcg_at_100_map.keys())\n",
    "    lens.sort()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for le in lens:\n",
    "        X.append(le)\n",
    "        ans = 0.0\n",
    "        for i in len_to_ndcg_at_100_map[le]: ans += float(i)\n",
    "        ans = ans / float(len(len_to_ndcg_at_100_map[le]))\n",
    "        Y.append(ans * 100.0)\n",
    "    Y_mine = []\n",
    "    prev_5 = []\n",
    "    for i in Y:\n",
    "        prev_5.append(i)\n",
    "        if len(prev_5) > 5: del prev_5[0]\n",
    "\n",
    "        temp = 0.0\n",
    "        for j in prev_5: temp += float(j)\n",
    "        temp = float(temp) / float(len(prev_5))\n",
    "        Y_mine.append(temp)\n",
    "    print(X)\n",
    "    print(Y_mine)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(X, Y_mine, label='SVAE')\n",
    "    plt.plot(X_mvae_ml1m, Y_mvae_ml1m, label='MVAE')\n",
    "    plt.plot(X_caser_ml1m, Y_caser_ml1m, label='CASER')\n",
    "#     plt.plot(X_mvae_netflix, Y_mvae_netflix, label='MVAE')\n",
    "    # plt.plot(X_caser_netflix, Y_caser_netflix, label='CASER')\n",
    "    plt.xlabel(\"Number of items in the fold-out set\")\n",
    "    plt.ylabel(\"Average NDCG@100\")\n",
    "    plt.title(\"Movielens\")\n",
    "    plt.savefig(\"saved_plots/seq_len_vs_ndcg_ml.pdf\")\n",
    "    #plt.xscale('log', basex=10)\n",
    "    #axes = plt.gca()\n",
    "    #axes.set_xlim([10, 500])\n",
    "    #axes.set_ylim([0, 50])\n",
    "\n",
    "    leg = plt.legend(loc='best', ncol=2)\n",
    "    \n",
    "    plt.show()\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NOTES:\n",
    "\n",
    "- Try having two different layers for mu and sigma\n",
    "- Never using dropout\n",
    "- Not using L2 Norm at input\n",
    "\n",
    "'''\n",
    "\n",
    "hyper_params = {\n",
    "    'data_base': '../saved_data/ml-1m/pro_sg/',\n",
    "    'project_name': 'svae_ml1m',\n",
    "#     'data_base': '../saved_data/ml-20m/pro_sg/',\n",
    "#     'project_name': 'svae_ml20m',\n",
    "#     'data_base': '../saved_data/netflix-full/pro_sg/',\n",
    "#     'project_name': 'svae_netflix_full',\n",
    "#     'data_base': '../saved_data/netflix-good-sample/pro_sg/',\n",
    "#     'project_name': 'svae_netflix_good_sample',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'history_split_test': [0.8, 0.2], # Part of test history to train on : Part of test history to test\n",
    "\n",
    "    'learning_rate': 0.01, # learning rate is required only if optimizer is adagrad\n",
    "    'optimizer': 'adam',\n",
    "    'weight_decay': float(5e-3),\n",
    "\n",
    "    'epochs': 25,\n",
    "    'batch_size': 256,\n",
    "    \n",
    "    'item_embed_size': 256,\n",
    "    'rnn_size': 200,\n",
    "    'sliding_window_size': 10,\n",
    "    'hidden_size': 150,\n",
    "    'latent_size': 64,\n",
    "    'loss_type': 'next_k', # [predict_next, same, prefix, postfix, exp_decay, next_k]\n",
    "    'next_k': 4,\n",
    "#     'conditional': True,\n",
    "#     'attention_context': 4,\n",
    "\n",
    "    'number_users_to_keep': 10000000000,\n",
    "    'batch_log_interval': 500,\n",
    "    'train_cp_users': 200,\n",
    "    'exploding_clip': 0.25,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_sliding_window_size_' + str(hyper_params['sliding_window_size'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "file_name += '_loss_type_' + str(hyper_params['loss_type'])\n",
    "if hyper_params['loss_type'] == 'next_k':\n",
    "    file_name += '_k_' + str(hyper_params['next_k'])\n",
    "#file_name += '_conditional_' + str(hyper_params['conditional'])\n",
    "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
    "file_name += '_rnn_size_' + str(hyper_params['rnn_size'])\n",
    "file_name += '_latent_size_' + str(hyper_params['latent_size'])\n",
    "#file_name += '_history_split_' + str(hyper_params['history_split_test'][0])\n",
    "\n",
    "hyper_params['log_file'] = '../saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = '../saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'train.csv')\n",
    "    lines_train = f.readlines()[1:]\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'test_tr.csv')\n",
    "    lines_test_tr = f.readlines()[1:]\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'test_te.csv')\n",
    "    lines_test_te = f.readlines()[1:]\n",
    "    \n",
    "    unique_sid = list()\n",
    "    with open(hyper_params['data_base'] + 'unique_sid.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "    num_items = len(unique_sid)\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "\n",
    "    train_reader = DataReader(hyper_params, lines_train, None, num_items, True)\n",
    "    test_reader = DataReader(hyper_params, lines_test_tr, lines_test_te, num_items, False)\n",
    "\n",
    "    return train_reader, test_reader, num_items\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, hyper_params, a, b, num_items, is_training):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        \n",
    "        num_users = 0\n",
    "        min_user = 1000000000000000000000000\n",
    "        for line in a:\n",
    "            line = line.strip().split(\",\")\n",
    "            num_users = max(num_users, int(line[0]))\n",
    "            min_user = min(min_user, int(line[0]))\n",
    "        num_users = num_users - min_user + 1\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.min_user = min_user\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        self.data_train = a\n",
    "        self.data_test = b\n",
    "        self.is_training = is_training\n",
    "        self.all_users = []\n",
    "        \n",
    "        self.prep()\n",
    "        self.number()\n",
    "\n",
    "    def prep(self):\n",
    "        self.data = []\n",
    "        for i in range(self.num_users): self.data.append([])\n",
    "            \n",
    "        for i in tqdm(range(len(self.data_train))):\n",
    "            line = self.data_train[i]\n",
    "            line = line.strip().split(\",\")\n",
    "            self.data[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
    "        \n",
    "        if self.is_training == False:\n",
    "            self.data_te = []\n",
    "            for i in range(self.num_users): self.data_te.append([])\n",
    "                \n",
    "            for i in tqdm(range(len(self.data_test))):\n",
    "                line = self.data_test[i]\n",
    "                line = line.strip().split(\",\")\n",
    "                self.data_te[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
    "        \n",
    "    def number(self):   \n",
    "        # Sliding window\n",
    "        self.num_b = 0\n",
    "        for user in range(min(len(self.data), self.hyper_params['number_users_to_keep'])):\n",
    "            self.num_b += max(len(self.data[user]) - self.hyper_params['sliding_window_size'] + 1, 0)\n",
    "        self.num_b = self.num_b // self.batch_size\n",
    "        \n",
    "        # Disjoint sliding window\n",
    "#         self.num_b = 0\n",
    "#         for user in range(min(len(self.data), self.hyper_params['number_users_to_keep'])):\n",
    "#             self.num_b += len(self.data[user]) // self.hyper_params['sliding_window_size']\n",
    "#         self.num_b = self.num_b // self.batch_size\n",
    "    \n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch_s = torch.zeros(self.batch_size, self.hyper_params['sliding_window_size'] - 1, self.num_items).cuda()\n",
    "        \n",
    "        user_iterate_order = list(range(len(self.data)))\n",
    "        # np.random.shuffle(user_iterate_order)\n",
    "        \n",
    "        for user in user_iterate_order:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "                        \n",
    "            sliding_window_size = self.hyper_params['sliding_window_size']\n",
    "            \n",
    "            # We now have a seq of size len(self.data[user])\n",
    "            # We want to send sequences, moving a window of fixed len over the seq\n",
    "            for start_ind in range(0, len(self.data[user]) - sliding_window_size):\n",
    "                \n",
    "                send_seq = self.data[user][start_ind:][:sliding_window_size]\n",
    "                \n",
    "                if self.hyper_params['loss_type'] == 'predict_next':\n",
    "                    for timestep in range(self.hyper_params['sliding_window_size'] - 1):\n",
    "                        y_batch_s[len(x_batch), timestep, :].scatter_(\n",
    "                            0, LongTensor([ i[0] for i in [ send_seq[timestep + 1] ] ]), 1.0\n",
    "                        )   \n",
    "                    x_batch.append([ i[0] for i in send_seq[:-1] ])\n",
    "                    \n",
    "                elif self.hyper_params['loss_type'] == 'next_k':\n",
    "                    for timestep in range(self.hyper_params['sliding_window_size'] - 1):\n",
    "                        y_batch_s[len(x_batch), timestep, :].scatter_(\n",
    "                            0, LongTensor([ i[0] for i in send_seq[timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
    "                        )\n",
    "                    x_batch.append([ i[0] for i in send_seq[:-1] ])\n",
    "                \n",
    "                if len(x_batch) == self.batch_size:\n",
    "\n",
    "                    yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False)\n",
    "\n",
    "                    x_batch = []\n",
    "                    y_batch_s = torch.zeros(self.batch_size, self.hyper_params['sliding_window_size'] - 1, self.num_items).cuda()\n",
    "\n",
    "    def iter_eval(self):\n",
    "\n",
    "        # test_batch_size = self.batch_size\n",
    "        test_batch_size = 1\n",
    "        \n",
    "        x_batch = []\n",
    "        test_movies, test_movies_r = [], []\n",
    "        now_at = 0\n",
    "        users_done = 0\n",
    "        \n",
    "        for user in range(len(self.data)):\n",
    "            \n",
    "            users_done += 1\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            \n",
    "            if self.is_training == True: \n",
    "                split = float(self.hyper_params['history_split_test'][0])\n",
    "                base_predictions_on = self.data[user][:int(split * len(self.data[user]))]\n",
    "                heldout_movies = self.data[user][int(split * len(self.data[user])):]\n",
    "            else:\n",
    "                base_predictions_on = self.data[user]\n",
    "                heldout_movies = self.data_te[user]\n",
    "                \n",
    "            if self.hyper_params['loss_type'] == 'predict_next':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "                y_batch_s = torch.zeros(test_batch_size, len(base_predictions_on) - 1, self.num_items).cuda()\n",
    "                for timestep in range(len(base_predictions_on) - 1):\n",
    "                    y_batch_s[now_at, timestep, :].scatter_(\n",
    "                        0, LongTensor([ i[0] for i in [ base_predictions_on[timestep + 1] ] ]), 1.0\n",
    "                    )\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'next_k':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "                y_batch_s = torch.zeros(test_batch_size, len(base_predictions_on) - 1, self.num_items).cuda()\n",
    "                for timestep in range(len(base_predictions_on) - 1):\n",
    "                    y_batch_s[now_at, timestep, :].scatter_(\n",
    "                        0, LongTensor([ i[0] for i in base_predictions_on[timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
    "                    )\n",
    "            \n",
    "            now_at += 1\n",
    "            \n",
    "            test_movies.append([ i[0] for i in heldout_movies ])\n",
    "            test_movies_r.append([ i[1] for i in heldout_movies ])\n",
    "            \n",
    "            if now_at == test_batch_size:\n",
    "                \n",
    "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False), \\\n",
    "                test_movies, test_movies_r\n",
    "                \n",
    "                x_batch = []\n",
    "                test_movies, test_movies_r = [], []\n",
    "                now_at = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['loss'] = 0.0\n",
    "    Ks = [10, 100]\n",
    "    for k in Ks: \n",
    "        metrics['NDCG@' + str(k)] = 0.0\n",
    "        metrics['HR@' + str(k)] = 0.0\n",
    "        metrics['Prec@' + str(k)] = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    total_ndcg = 0.0\n",
    "    \n",
    "    len_to_ndcg_at_100_map = {}\n",
    "\n",
    "    for x, y_s, test_movies, test_movies_r in reader.iter_eval():\n",
    "        batch += 1\n",
    "        if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
    "\n",
    "        decoder_output, z_mean, z_log_sigma = model(x)\n",
    "        \n",
    "        metrics['loss'] += criterion(decoder_output, z_mean, z_log_sigma, y_s, 0.2).data[0]\n",
    "        \n",
    "        # decoder_output[X.nonzero()] = -np.inf\n",
    "        decoder_output = decoder_output.data\n",
    "        \n",
    "        x_scattered = torch.zeros(decoder_output.shape[0], decoder_output.shape[2]).cuda()\n",
    "        for bnum in range(int(decoder_output.shape[0])):\n",
    "            x_scattered[bnum, :].scatter_(0, x[bnum].data, 1.0)\n",
    "            # If loss type is predict next, the last element in the train sequence is not included in x\n",
    "            if hyper_params['loss_type'] == 'predict_next': x_scattered[bnum, y[bnum][-1]] = 1.0\n",
    "        \n",
    "        last_predictions = decoder_output[:, -1, :] - \\\n",
    "        (torch.abs(decoder_output[:, -1, :] * x_scattered) * 100000000)\n",
    "        \n",
    "        for batch_num in range(last_predictions.shape[0]):\n",
    "            predicted_scores = last_predictions[batch_num]\n",
    "            actual_movies_watched = test_movies[batch_num]\n",
    "            actual_movies_ratings = test_movies_r[batch_num]\n",
    "                    \n",
    "            # Calculate NDCG\n",
    "            _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
    "            for k in Ks:\n",
    "                best = 0.0\n",
    "                now_at = 0.0\n",
    "                dcg = 0.0\n",
    "                hr = 0.0\n",
    "                \n",
    "                rec_list = list(argsorted[:k].cpu().numpy())\n",
    "                for m in range(len(actual_movies_watched)):\n",
    "                    movie = actual_movies_watched[m]\n",
    "                    now_at += 1.0\n",
    "                    if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
    "                    \n",
    "                    if movie not in rec_list: continue\n",
    "                    hr += 1.0\n",
    "                    dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
    "                \n",
    "                metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
    "                metrics['HR@' + str(k)] += float(hr) / float(len(actual_movies_watched))\n",
    "                metrics['Prec@' + str(k)] += float(hr) / float(k)\n",
    "                \n",
    "                if k == 100:\n",
    "                    seq_len = int(len(actual_movies_watched)) + int(x[batch_num].shape[0]) + 1\n",
    "                    if seq_len not in len_to_ndcg_at_100_map: len_to_ndcg_at_100_map[seq_len] = []\n",
    "                    len_to_ndcg_at_100_map[seq_len].append(float(dcg) / float(best))\n",
    "                \n",
    "            total_ndcg += 1.0\n",
    "    \n",
    "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
    "    metrics['loss'] = round(metrics['loss'], 4)\n",
    "    \n",
    "    for k in Ks:\n",
    "        metrics['NDCG@' + str(k)] = round((100.0 * metrics['NDCG@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['HR@' + str(k)] = round((100.0 * metrics['HR@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['Prec@' + str(k)] = round((100.0 * metrics['Prec@' + str(k)]) / float(total_ndcg), 4)\n",
    "        \n",
    "    return metrics, len_to_ndcg_at_100_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['rnn_size'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder = Decoder(hyper_params)\n",
    "        \n",
    "        # No +1 means can never pad, hence bsz has to be equal 1\n",
    "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            hyper_params['item_embed_size'], hyper_params['rnn_size'], \n",
    "            batch_first=True, num_layers=1\n",
    "        )\n",
    "        \n",
    "        self.layer_temp = nn.Linear(hyper_params['hidden_size'], 2 * hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self.layer_temp.weight)\n",
    "        \n",
    "        self.rnn_hidden = None\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        temp_out = self.layer_temp(h_enc)\n",
    "        \n",
    "        mu = temp_out[:, :self.hyper_params['latent_size']]\n",
    "        log_sigma = temp_out[:, self.hyper_params['latent_size']:]\n",
    "        \n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "        if is_cuda_available: std_z = std_z.cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_log_sigma = log_sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_shape = x.shape\n",
    "        x = x.view(-1)\n",
    "        \n",
    "        x = self.item_embed(x)\n",
    "        x = x.view(in_shape[0], in_shape[1], -1)\n",
    "        \n",
    "#         if self.rnn_hidden is None:\n",
    "        rnn_out, _ = self.gru(x)\n",
    "#         else:\n",
    "#             self.rnn_hidden = Variable(self.rnn_hidden.data)\n",
    "#             rnn_out, self.rnn_hidden = self.gru(x, self.rnn_hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(in_shape[0] * in_shape[1], -1)\n",
    "        \n",
    "        enc_out = self.encoder(rnn_out)\n",
    "        sampled_z = self.sample_latent(enc_out)\n",
    "        \n",
    "#         if self.hyper_params['conditional'] == True:\n",
    "#             mult_matrix = torch.zeros(in_shape[0], in_shape[1], in_shape[1]).cuda()\n",
    "#             for b in range(in_shape[0]):\n",
    "#                 for i in range(in_shape[1]):\n",
    "#                     num_one = i + 1 - max(i-self.hyper_params['attention_context']+1, 0)\n",
    "#                     to_put = 1.0 / float(num_one)\n",
    "#                     for j in range(max(i-self.hyper_params['attention_context']+1, 0), i+1): mult_matrix[b, i, j] = to_put\n",
    "#             mult_matrix = Variable(mult_matrix.view(in_shape[0]*in_shape[1], in_shape[1]))\n",
    "            \n",
    "#             print(mult_matrix.shape)\n",
    "#             print(sampled_z.shape)\n",
    "#             sampled_z = torch.mm(mult_matrix, sampled_z)\n",
    "        \n",
    "        dec_out = self.decoder(sampled_z)\n",
    "        dec_out = dec_out.view(in_shape[0], in_shape[1], -1)\n",
    "                              \n",
    "        return dec_out, self.z_mean, self.z_log_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def forward(self, decoder_output, mu_q, logvar_q, y_true_s, anneal):\n",
    "        # Calculate KL Divergence loss\n",
    "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
    "    \n",
    "        # decoder_output shape : [batch_size, seq_len, all_items]\n",
    "        dec_shape = decoder_output.shape\n",
    "\n",
    "        decoder_output = F.log_softmax(decoder_output, -1)\n",
    "        num_ones = float(torch.sum(y_true_s[0, 0]))\n",
    "        \n",
    "        likelihood = torch.sum(\n",
    "            -1.0 * y_true_s.view(dec_shape[0] * dec_shape[1], -1) * \\\n",
    "            decoder_output.view(dec_shape[0] * dec_shape[1], -1)\n",
    "        ) / (float(self.hyper_params['batch_size']) * num_ones)\n",
    "        \n",
    "        final = (anneal * kld) + (1.0 * likelihood)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435180/435180 [00:00<00:00, 646177.82it/s]\n",
      "100%|██████████| 54859/54859 [00:00<00:00, 952302.47it/s]\n",
      "100%|██████████| 14097/14097 [00:00<00:00, 177025.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Simulation run on: 2018-10-18 13:54:54.022814\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches: 1540\n",
      "Number of test batches:  188\n",
      "Total Items: 3483\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=200, out_features=150, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear1): Linear(in_features=64, out_features=150, bias=True)\n",
      "    (linear2): Linear(in_features=150, out_features=3483, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (item_embed): Embedding(3483, 256)\n",
      "  (gru): GRU(256, 200, batch_first=True)\n",
      "  (layer_temp): Linear(in_features=150, out_features=128, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n",
      "| epoch   1 |   500/ 1540 batches | ms/batch 99.97 | loss 54.2973\n",
      "| epoch   1 |  1000/ 1540 batches | ms/batch 100.25 | loss 52.2702\n",
      "| epoch   1 |  1500/ 1540 batches | ms/batch 100.27 | loss 51.8626\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Exiting from training early\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../saved_models/svae_ml1m_model_optimizer_adam_sliding_window_size_10_weight_decay_0.005_loss_type_next_k_k_4_item_embed_size_256_rnn_size_200_latent_size_64.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f046f8ddf7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Checking metrics on best saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_to_ndcg_at_100_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../saved_models/svae_ml1m_model_optimizer_adam_sliding_window_size_10_weight_decay_0.005_loss_type_next_k_k_4_item_embed_size_256_rnn_size_200_latent_size_64.pt'"
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "    total_anneal_steps = 200000\n",
    "    anneal = 0.0\n",
    "    update_count = 0.0\n",
    "    anneal_cap = 0.2\n",
    "\n",
    "    for x, y_s in reader.iter():\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_output, z_mean, z_log_sigma = model(x)\n",
    "        \n",
    "        loss = criterion(decoder_output, z_mean, z_log_sigma, y_s, anneal)\n",
    "        loss.backward()\n",
    "        \n",
    "        # nn.utils.clip_grad_norm(model.parameters(), hyper_params['exploding_clip'])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "        if total_anneal_steps > 0:\n",
    "            anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "        else:\n",
    "            anneal = anneal_cap\n",
    "        update_count += 1.0\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_items = load_data(hyper_params)\n",
    "# print(train_reader.data[:10])\n",
    "# print(test_reader.data[:10])\n",
    "# print(test_reader.data_te[:10])\n",
    "hyper_params['total_items'] = total_items\n",
    "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "# file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        model.rnn_hidden = None\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "        metrics, _ = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "        string = \"\"\n",
    "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics, len_to_ndcg_at_100_map = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        # Plot sequence length vs NDCG@100 graph\n",
    "        # plot_len_vs_ndcg(len_to_ndcg_at_100_map)\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "# Checking metrics on best saved model\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics, len_to_ndcg_at_100_map = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "# Plot sequence length vs NDCG@100 graph\n",
    "plot_len_vs_ndcg(len_to_ndcg_at_100_map)\n",
    "\n",
    "# Plot Traning graph\n",
    "f = open(model.hyper_params['log_file'])\n",
    "lines = f.readlines()\n",
    "lines.reverse()\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[:10] == 'Simulation' and len(train) > 5: break\n",
    "    elif line[:10] == 'Simulation' and len(train) <= 5: train, test = [], []\n",
    "        \n",
    "    if line[2:5] == 'end' and line[-6:-2] == 'TEST': test.append(line.strip().split(\"|\"))\n",
    "    elif line[2:5] == 'end' and line[-7:-2] == 'TRAIN': train.append(line.strip().split(\"|\"))\n",
    "\n",
    "train.reverse()\n",
    "test.reverse()\n",
    "\n",
    "train_cp, train_ndcg = [], []\n",
    "test_cp, test_ndcg = [], []\n",
    "\n",
    "for i in train:\n",
    "    train_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    train_ndcg.append(float(i[-3].split('=')[1].split(' ')[1]))\n",
    "    \n",
    "for i in test:\n",
    "    test_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    test_ndcg.append(float(i[-3].split('=')[1].split(' ')[1]))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_ndcg, label='Train set')\n",
    "plt.plot(test_ndcg, label='Test set')\n",
    "plt.ylabel(\"NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "leg = plt.legend(loc='best', ncol=2)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
