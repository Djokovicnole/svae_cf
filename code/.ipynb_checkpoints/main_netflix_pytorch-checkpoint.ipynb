{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    # 'data_base': 'saved_data/netflix/pro_sg/',\n",
    "    # 'project_name': 'netflix_pytorch_netflix',\n",
    "    # 'data_base': 'saved_data/pro_sg/',\n",
    "    # 'project_name': 'netflix_pytorch_ml1m',\n",
    "    # 'data_base': 'saved_data/netflix-full/pro_sg/',\n",
    "    # 'project_name': 'netflix_pytorch_netflix_full',\n",
    "    'data_base': '../saved_data/netflix-good-sample/pro_sg/',\n",
    "    'project_name': 'netflix_pytorch_netflix_good_sample',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'history_split_test': [0.8, 0.2], # Part of test history to train on : Part of test history to test\n",
    "\n",
    "    'learning_rate': 0.01, # if optimizer is adadelta, learning rate is not required\n",
    "    'optimizer': 'adam',\n",
    "    'weight_decay': float(1e-2),\n",
    "\n",
    "    'epochs': 50,\n",
    "    'batch_size': 512,\n",
    "\n",
    "    'hidden_size': 600,\n",
    "    'latent_size': 200,\n",
    "\n",
    "    'number_users_to_keep': 100000000000,\n",
    "    'batch_log_interval': 100,\n",
    "    'train_cp_users': 200,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "\n",
    "hyper_params['log_file'] = '../saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = '../saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'train.csv')\n",
    "    lines_train = f.readlines()[1:]\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'test_tr.csv')\n",
    "    lines_test_tr = f.readlines()[1:]\n",
    "    \n",
    "    f = open(hyper_params['data_base'] + 'test_te.csv')\n",
    "    lines_test_te = f.readlines()[1:]\n",
    "    \n",
    "    unique_sid = list()\n",
    "    with open(hyper_params['data_base'] + 'unique_sid.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "    num_items = len(unique_sid)\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "\n",
    "    train_reader = DataReader(hyper_params, lines_train, None, num_items, True)\n",
    "    test_reader = DataReader(hyper_params, lines_test_tr, lines_test_te, num_items, False)\n",
    "\n",
    "    return train_reader, test_reader, num_items\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, hyper_params, a, b, num_items, is_training):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        \n",
    "        num_users = 0\n",
    "        min_user = 1000000000000000000000000\n",
    "        for line in a:\n",
    "            line = line.strip().split(\",\")\n",
    "            num_users = max(num_users, int(line[0]))\n",
    "            min_user = min(min_user, int(line[0]))\n",
    "        num_users = num_users - min_user + 1\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.min_user = min_user\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        self.data_train = a\n",
    "        self.data_test = b\n",
    "        self.is_training = is_training\n",
    "        self.all_users = []\n",
    "        \n",
    "        self.prep()\n",
    "        self.number()\n",
    "\n",
    "    def prep(self):\n",
    "        self.data = []\n",
    "        for i in range(self.num_users): self.data.append([])\n",
    "            \n",
    "        for i in tqdm(range(len(self.data_train))):\n",
    "            line = self.data_train[i]\n",
    "            line = line.strip().split(\",\")\n",
    "            self.data[int(line[0]) - self.min_user].append(int(line[1]))\n",
    "        \n",
    "        if self.is_training == False:\n",
    "            self.data_te = []\n",
    "            for i in range(self.num_users): self.data_te.append([])\n",
    "                \n",
    "            for i in tqdm(range(len(self.data_test))):\n",
    "                line = self.data_test[i]\n",
    "                line = line.strip().split(\",\")\n",
    "                self.data_te[int(line[0]) - self.min_user].append(int(line[1]))\n",
    "        \n",
    "    def number(self):\n",
    "        self.num_b = int(min(len(self.data), self.hyper_params['number_users_to_keep']) / self.batch_size)\n",
    "        \n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        #y_batch = []\n",
    "        done_now = 0\n",
    "        \n",
    "        user_iterate_order = list(range(len(self.data)))\n",
    "        np.random.shuffle(user_iterate_order)\n",
    "        \n",
    "        for user in user_iterate_order:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "            \n",
    "            # print(self.data[user])\n",
    "            x_batch[done_now, :].scatter_(0, LongTensor(self.data[user]), 1.0)\n",
    "            y_batch_s[done_now, :].scatter_(0, LongTensor(self.data[user]), 1.0)\n",
    "            #y_batch.append([ i[0] for i in self.data[user] ])\n",
    "            done_now += 1\n",
    "    \n",
    "            if done_now == self.batch_size:\n",
    "\n",
    "                yield Variable(x_batch), Variable(y_batch_s, requires_grad=False)\n",
    "                #yield Variable(x_batch), y_batch\n",
    "\n",
    "                x_batch = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                #y_batch = []\n",
    "                done_now = 0\n",
    "\n",
    "    def iter_eval(self):\n",
    "\n",
    "        x_batch = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        #y_batch = []\n",
    "        test_movies, test_movies_r = [], []\n",
    "        now_at = 0\n",
    "        \n",
    "        for user in range(len(self.data)):\n",
    "            \n",
    "            if self.is_training == True: \n",
    "                split = float(self.hyper_params['history_split_test'][0])\n",
    "                base_predictions_on = self.data[user][:int(split * len(self.data[user]))]\n",
    "                heldout_movies = self.data[user][int(split * len(self.data[user])):]\n",
    "            else:\n",
    "                base_predictions_on = self.data[user]\n",
    "                heldout_movies = self.data_te[user]\n",
    "                \n",
    "            x_batch[now_at, :].scatter_(0, LongTensor([ i for i in base_predictions_on ]), 1.0)\n",
    "            y_batch_s[now_at, :].scatter_(0, LongTensor([ i for i in base_predictions_on ]), 1.0)\n",
    "            #y_batch.append([ i[0] for i in base_predictions_on ])\n",
    "            now_at += 1\n",
    "            \n",
    "            test_movies.append([ i for i in heldout_movies ])\n",
    "            test_movies_r.append([ 1 for i in heldout_movies ])\n",
    "            \n",
    "            if now_at == self.batch_size:\n",
    "                \n",
    "                yield Variable(x_batch), Variable(y_batch_s, requires_grad=False), \\\n",
    "                test_movies, test_movies_r\n",
    "                #yield Variable(x_batch), y_batch, test_movies, test_movies_r\n",
    "                \n",
    "                x_batch = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                #y_batch = []\n",
    "                test_movies, test_movies_r = [], []\n",
    "                now_at = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['loss'] = 0.0\n",
    "    Ks = [10, 100]\n",
    "    for k in Ks: \n",
    "        metrics['NDCG@' + str(k)] = 0.0\n",
    "        metrics['HR@' + str(k)] = 0.0\n",
    "        metrics['Prec@' + str(k)] = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    total_ndcg = 0.0\n",
    "    \n",
    "    len_to_ndcg_at_100_map = {}\n",
    "\n",
    "    for x, y, test_movies, test_movies_r in reader.iter_eval():\n",
    "        batch += 1\n",
    "        if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
    "\n",
    "        decoder_output, z_mean, z_sigma = model(x)\n",
    "        \n",
    "        metrics['loss'] += criterion(decoder_output, z_mean, z_sigma, y, 0.2).data[0]\n",
    "        \n",
    "        # decoder_output[X.nonzero()] = -np.inf\n",
    "        last_predictions = decoder_output - (torch.abs(decoder_output * x) * 100000000)\n",
    "        \n",
    "        for batch_num in range(last_predictions.shape[0]):\n",
    "            predicted_scores = last_predictions[batch_num]\n",
    "            actual_movies_watched = test_movies[batch_num]\n",
    "            actual_movies_ratings = test_movies_r[batch_num]\n",
    "                    \n",
    "            # Calculate NDCG\n",
    "            _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
    "            for k in Ks:\n",
    "                best = 0.0\n",
    "                now_at = 0.0\n",
    "                dcg = 0.0\n",
    "                hr = 0.0\n",
    "                \n",
    "                rec_list = list(argsorted[:k].data.cpu().numpy())\n",
    "                for m in range(len(actual_movies_watched)):\n",
    "                    movie = actual_movies_watched[m]\n",
    "                    now_at += 1.0\n",
    "                    if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
    "                    \n",
    "                    if movie not in rec_list: continue\n",
    "                    hr += 1.0\n",
    "                    dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
    "                \n",
    "                metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
    "                metrics['HR@' + str(k)] += float(hr) / float(len(actual_movies_watched))\n",
    "                metrics['Prec@' + str(k)] += float(hr) / float(k)\n",
    "                \n",
    "                if k == 100:\n",
    "                    seq_len = int(len(actual_movies_watched)) + int((x[batch_num].nonzero()).shape[0])\n",
    "                    if seq_len not in len_to_ndcg_at_100_map: len_to_ndcg_at_100_map[seq_len] = []\n",
    "                    len_to_ndcg_at_100_map[seq_len].append(float(dcg) / float(best))\n",
    "                \n",
    "            total_ndcg += 1.0\n",
    "    \n",
    "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
    "    metrics['loss'] = round(metrics['loss'], 4)\n",
    "    \n",
    "    for k in Ks:\n",
    "        metrics['NDCG@' + str(k)] = round((100.0 * metrics['NDCG@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['HR@' + str(k)] = round((100.0 * metrics['HR@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['Prec@' + str(k)] = round((100.0 * metrics['Prec@' + str(k)]) / float(total_ndcg), 4)\n",
    "\n",
    "    return metrics, len_to_ndcg_at_100_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['total_items'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "#         x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder = Decoder(hyper_params)\n",
    "        \n",
    "        self.layer_temp = nn.Linear(hyper_params['hidden_size'], 2*hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self.layer_temp.weight)\n",
    "        \n",
    "#         self._enc_mu = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "#         self._enc_log_sigma = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "#         nn.init.xavier_normal(self._enc_mu.weight)\n",
    "#         nn.init.xavier_normal(self._enc_log_sigma.weight)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l2_norm = LayerNorm(hyper_params['total_items'])\n",
    "#         self.l2_norm = nn.BatchNorm1d(hyper_params['total_items'])\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "#         mu = self._enc_mu(h_enc)\n",
    "#         log_sigma = self._enc_log_sigma(h_enc)\n",
    "        temp_out = self.layer_temp(h_enc)\n",
    "        mu = temp_out[:, :self.hyper_params['latent_size']]\n",
    "        log_sigma = temp_out[:, self.hyper_params['latent_size']:]\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "        if is_cuda_available: std_z = std_z.cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = log_sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l2_norm(x)\n",
    "        # x = self.dropout(x)\n",
    "        h_enc = self.encoder(x)\n",
    "        z = self.sample_latent(h_enc)\n",
    "        dec = self.decoder(z)\n",
    "                              \n",
    "        return dec, self.z_mean, self.z_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "        self.cce = nn.CrossEntropyLoss(size_average=True)\n",
    "\n",
    "    def forward(self, decoder_output, mu_q, logvar_q, y_true_s, anneal):\n",
    "        \n",
    "#         mean_sq = mu_q * mu_q\n",
    "#         stddev_sq = std_q * std_q\n",
    "#         std_q = tf.exp(0.5 * logvar_q)\n",
    "#         print(mean_sq.shape)\n",
    "#         print(stddev_sq.shape)\n",
    "#         kld = torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n",
    "        \n",
    "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
    "    \n",
    "#         likelihood = None\n",
    "#         print(torch.sum(decoder_output[0]))\n",
    "#         decoder_output = F.softmax(decoder_output, -1)\n",
    "#         print(torch.sum(decoder_output[0]))\n",
    "#         for b in range(len(y_true_s)):\n",
    "#             for i in y_true_s[b]:\n",
    "#                 if likelihood is None: likelihood = -1.0 * torch.log(decoder_output[b, i])\n",
    "#                 else: likelihood = likelihood - torch.log(decoder_output[b, i])\n",
    "#         likelihood = likelihood / float(len(y_true_s))\n",
    "    \n",
    "        decoder_output = F.log_softmax(decoder_output, -1)\n",
    "        temp = y_true_s * decoder_output\n",
    "        likelihood = torch.mean(-torch.sum(temp, -1))\n",
    "        \n",
    "        final = (anneal * kld) + (1.0 * likelihood)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11252374/11252374 [00:10<00:00, 1065784.17it/s]\n",
      "100%|██████████| 1202422/1202422 [00:01<00:00, 1041044.50it/s]\n",
      "100%|██████████| 304605/304605 [00:00<00:00, 995861.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Simulation run on: 2018-10-15 07:14:15.126089\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches:  116\n",
      "Number of test batches:   15\n",
      "Total Items: 17647\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=17647, out_features=600, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear1): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (linear2): Linear(in_features=600, out_features=17647, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (layer_temp): Linear(in_features=600, out_features=400, bias=True)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (l2_norm): LayerNorm(\n",
      "  )\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "    total_anneal_steps = 200000\n",
    "    anneal = 0.0\n",
    "    update_count = 0.0\n",
    "    anneal_cap = 0.2\n",
    "\n",
    "    for x, y in reader.iter():\n",
    "        # print(x[0])\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_output, z_mean, z_sigma = model(x)\n",
    "        \n",
    "        loss = criterion(decoder_output, z_mean, z_sigma, y, anneal)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "        if total_anneal_steps > 0:\n",
    "            anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "        else:\n",
    "            anneal = anneal_cap\n",
    "        update_count += 1.0\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_items = load_data(hyper_params)\n",
    "# print(train_reader.data[:10])\n",
    "# print(test_reader.data[:10])\n",
    "# print(test_reader.data_te[:10])\n",
    "hyper_params['total_items'] = total_items\n",
    "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "# file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "        metrics, _ = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "        string = \"\"\n",
    "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics, len_to_ndcg_at_100_map = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        # Plot sequence length vs NDCG@100 graph\n",
    "        # plot_len_vs_ndcg(len_to_ndcg_at_100_map)\n",
    "        lens = list(len_to_ndcg_at_100_map.keys())\n",
    "        lens.sort()\n",
    "        X = []\n",
    "        Y = []\n",
    "        for le in lens:\n",
    "            X.append(le)\n",
    "            ans = 0.0\n",
    "            for i in len_to_ndcg_at_100_map[le]: ans += float(i)\n",
    "            ans = ans / float(len(len_to_ndcg_at_100_map[le]))\n",
    "            Y.append(ans * 100.0)\n",
    "        Y_mine = []\n",
    "        prev_5 = []\n",
    "        for i in Y:\n",
    "            prev_5.append(i)\n",
    "            if len(prev_5) > 5: del prev_5[0]\n",
    "\n",
    "            temp = 0.0\n",
    "            for j in prev_5: temp += float(j)\n",
    "            temp = float(temp) / float(len(prev_5))\n",
    "            Y_mine.append(temp)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        print(X)\n",
    "        print(Y_mine)\n",
    "        plt.plot(X, Y_mine)\n",
    "        plt.xlabel(\"Sequence Lengths\")\n",
    "        plt.ylabel(\"NDCG@100\")\n",
    "#         plt.show()\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "# Checking metrics on best saved model\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics, len_to_ndcg_at_100_map = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "# Plot sequence length vs NDCG@100 graph\n",
    "# plot_len_vs_ndcg(len_to_ndcg_at_100_map)\n",
    "lens = list(len_to_ndcg_at_100_map.keys())\n",
    "lens.sort()\n",
    "X = []\n",
    "Y = []\n",
    "for le in lens:\n",
    "    X.append(le)\n",
    "    ans = 0.0\n",
    "    for i in len_to_ndcg_at_100_map[le]: ans += float(i)\n",
    "    ans = ans / float(len(len_to_ndcg_at_100_map[le]))\n",
    "    Y.append(ans * 100.0)\n",
    "Y_mine = []\n",
    "prev_5 = []\n",
    "for i in Y:\n",
    "    prev_5.append(i)\n",
    "    if len(prev_5) > 5: del prev_5[0]\n",
    "\n",
    "    temp = 0.0\n",
    "    for j in prev_5: temp += float(j)\n",
    "    temp = float(temp) / float(len(prev_5))\n",
    "    Y_mine.append(temp)\n",
    "plt.figure(figsize=(12, 5))\n",
    "print(X)\n",
    "print(Y_mine)\n",
    "plt.plot(X, Y_mine)\n",
    "plt.xlabel(\"Sequence Lengths\")\n",
    "plt.ylabel(\"NDCG@100\")\n",
    "pass\n",
    "\n",
    "# Plot Traning graph\n",
    "f = open(model.hyper_params['log_file'])\n",
    "lines = f.readlines()\n",
    "lines.reverse()\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[:10] == 'Simulation' and len(train) > 0: break\n",
    "    if line[2:5] == 'end' and line[-6:-2] == 'TEST': test.append(line.strip().split(\"|\"))\n",
    "    elif line[2:5] == 'end' and line[-7:-2] == 'TRAIN': train.append(line.strip().split(\"|\"))\n",
    "\n",
    "train.reverse()\n",
    "test.reverse()\n",
    "\n",
    "train_cp, train_ndcg = [], []\n",
    "test_cp, test_ndcg = [], []\n",
    "\n",
    "for i in train:\n",
    "    train_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    train_ndcg.append(float(i[-2].split('=')[1].split(' ')[1]))\n",
    "    \n",
    "for i in test:\n",
    "    test_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    test_ndcg.append(float(i[-2].split('=')[1].split(' ')[1]))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_ndcg, label='Train')\n",
    "plt.plot(test_ndcg, label='Test')\n",
    "plt.ylabel(\"NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "leg = plt.legend(loc='best', ncol=2)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
