{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "#     'data_base': 'saved_data/pro_sg/',\n",
    "#     'project_name': 'ranking_vae_single_score_ml1m',\n",
    "    'data_base': 'saved_data/netflix-good-sample/pro_sg/',\n",
    "    'project_name': 'ranking_vae_netflix_good_sample',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'data_split': [0.8, 0.2], # Train : Test\n",
    "\n",
    "    'learning_rate': 0.05, # if optimizer is adadelta, learning rate is not required\n",
    "    'optimizer': 'adam',\n",
    "    'loss_type': 'hinge',\n",
    "    'm_loss': float(1),\n",
    "    'weight_decay': float(1e-6),\n",
    "\n",
    "    'epochs': 50,\n",
    "    'batch_size': 512,\n",
    "\n",
    "    'user_embed_size': 128,\n",
    "    'item_embed_size': 128,\n",
    "    \n",
    "    'hidden_size': 100,\n",
    "    'latent_size': 64,\n",
    "\n",
    "    'number_users_to_keep': 100000,\n",
    "    'batch_log_interval': 2000,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] != 'adadelta':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_user_embed_size_' + str(hyper_params['user_embed_size'])\n",
    "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "\n",
    "hyper_params['log_file'] = 'saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = 'saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):   \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    f = open(hyper_params['data_base'] + \"train.csv\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    item_hist = {}\n",
    "    max_item = 0\n",
    "    max_user = 0\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(\",\")\n",
    "        if line[0] not in item_hist: item_hist[line[0]] = []\n",
    "        item_hist[line[0]].append(int(line[1]))\n",
    "        max_item = max(max_item, int(line[1]))\n",
    "        max_user = max(max_user, int(line[0]))\n",
    "\n",
    "    f = open(hyper_params['data_base'] + \"test_tr.csv\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    test_tr = {}\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(\",\")\n",
    "        if line[0] not in test_tr: test_tr[line[0]] = []\n",
    "        test_tr[line[0]].append(int(line[1]))\n",
    "        \n",
    "        # ADDING FOLD-IN SET TO TRAINING\n",
    "        if line[0] not in item_hist: item_hist[line[0]] = []\n",
    "        item_hist[line[0]].append(int(line[1]))\n",
    "\n",
    "        max_item = max(max_item, int(line[1]))\n",
    "        max_user = max(max_user, int(line[0]))\n",
    "\n",
    "    # Sample negs\n",
    "    number_negs = 5\n",
    "    negs = {}\n",
    "    for user in item_hist:\n",
    "        negs[user] = set()\n",
    "        while len(negs[user]) < number_negs:\n",
    "            random_neg = random.randint(0, max_item)\n",
    "            if random_neg not in item_hist[user]: negs[user].add(random_neg)\n",
    "        negs[user] = list(negs[user])\n",
    "\n",
    "    f = open(hyper_params['data_base'] + \"test_te.csv\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    test = {}\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(\",\")\n",
    "        if line[0] not in test: test[line[0]] = []\n",
    "        test[line[0]].append(int(line[1]))\n",
    "        max_item = max(max_item, int(line[1]))\n",
    "        max_user = max(max_user, int(line[0]))\n",
    "        \n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "    \n",
    "    train_reader = DataReader(hyper_params, item_hist, negs, max_user+1, max_item+1, True)\n",
    "    test_reader = DataReader(hyper_params, test_tr, test, max_user+1, max_item+1, False)\n",
    "\n",
    "    return train_reader, test_reader, max_user+1, max_item+1\n",
    "    \n",
    "#     return item_hist, negs, test_tr, test, max_user+1, max_item+1\n",
    "    \n",
    "#     train = load_obj_json(hyper_params['data_base'] + 'train_ranking_vae')\n",
    "#     test = load_obj_json(hyper_params['data_base'] + 'test_ranking_vae')\n",
    "#     user_hist = load_obj_json(hyper_params['data_base'] + 'user_hist_ranking_vae')\n",
    "#     item_hist = load_obj_json(hyper_params['data_base'] + 'item_hist_ranking_vae')\n",
    "\n",
    "#     train_reader = DataReader(hyper_params, train, len(user_hist), item_hist, True)\n",
    "#     test_reader = DataReader(hyper_params, test, len(user_hist), item_hist, False)\n",
    "\n",
    "#     return train_reader, test_reader, len(user_hist), len(item_hist)\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, hyper_params, a, b, num_users, num_items, is_training):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        if is_training == False:\n",
    "            self.test_tr = a\n",
    "            self.test_te = b\n",
    "        else:\n",
    "            self.data = a\n",
    "            self.negs = b\n",
    "            self.number()\n",
    "\n",
    "    def number(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "\n",
    "        x_batch_user = []\n",
    "        x_batch_item = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                for ii in range(len(self.negs[user])):\n",
    "\n",
    "                    x_batch_user.append(0)\n",
    "\n",
    "                    x_batch_item.append(0)\n",
    "                    x_batch_item.append(0)\n",
    "\n",
    "                    if len(x_batch_user) == self.batch_size:\n",
    "\n",
    "                        count += 1\n",
    "                        \n",
    "                        x_batch_user = []\n",
    "                        x_batch_item = []\n",
    "\n",
    "        self.num_b = count\n",
    "\n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch_user = []\n",
    "        x_batch_item = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                for ii in range(len(self.negs[user])):\n",
    "\n",
    "                    x_batch_user.append(int(user))\n",
    "\n",
    "                    x_batch_item.append(self.data[user][i])\n",
    "                    x_batch_item.append(self.negs[user][ii])\n",
    "\n",
    "                    if len(x_batch_user) == self.batch_size:\n",
    "\n",
    "                        yield Variable(LongTensor(x_batch_user)), Variable(LongTensor(x_batch_item[::2])), Variable(LongTensor(x_batch_item[1::2]))\n",
    "                        \n",
    "                        x_batch_user = []\n",
    "                        x_batch_item = []\n",
    "\n",
    "    def iter_eval(self):\n",
    "        all_users = list(self.test_te.keys())\n",
    "        users_done = 0\n",
    "        \n",
    "        for user_now in tqdm(range(len(all_users))):\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "            \n",
    "            user = all_users[user_now]\n",
    "\n",
    "            yield int(user), self.test_tr[user], self.test_te[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_int(a):\n",
    "    if float(a.data) < 0.0: return -1\n",
    "    if float(a.data) > 0.0: return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_ndcg(model, criterion, reader, hyper_params):\n",
    "    model.eval()\n",
    "\n",
    "    ret = 0.0\n",
    "    \n",
    "    Ks = [10, 100]\n",
    "    metrics = {}\n",
    "    for k in Ks:\n",
    "        metrics['NDCG@' + str(k)] = 0.0\n",
    "        metrics['HR@' + str(k)] = 0.0\n",
    "        metrics['Prec@' + str(k)] = 0.0\n",
    "\n",
    "    user_done = 0\n",
    "    total_ndcg = 0.0\n",
    "\n",
    "    for u, x_tr, x_te in reader.iter_eval():\n",
    "        user_done += 1\n",
    "        \n",
    "        x_user = [ u for i in range(hyper_params['total_items']) ]\n",
    "        x_item = list(range(hyper_params['total_items']))\n",
    "        \n",
    "        x_user = Variable(LongTensor(x_user))\n",
    "        x_item = Variable(LongTensor(x_item))\n",
    "\n",
    "        _, scores = model(x_user, x_item)\n",
    "\n",
    "        scores = scores.data\n",
    "        scores[LongTensor(x_tr)] = -np.inf\n",
    "        \n",
    "        _, argsorted = torch.sort(-1.0 * scores)\n",
    "        for k in Ks:\n",
    "            best = 0.0\n",
    "            now_at = 0.0\n",
    "            dcg = 0.0\n",
    "            hr = 0.0\n",
    "\n",
    "            rec_list = list(argsorted[:k].cpu().numpy())\n",
    "            for m in range(len(x_te)):\n",
    "                movie = x_te[m]\n",
    "                now_at += 1.0\n",
    "                if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
    "\n",
    "                if movie not in rec_list: continue\n",
    "                hr += 1.0\n",
    "                dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
    "\n",
    "            metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
    "            metrics['HR@' + str(k)] += float(hr) / float(len(x_te))\n",
    "            metrics['Prec@' + str(k)] += float(hr) / float(k)\n",
    "\n",
    "        total_ndcg += 1.0\n",
    "    \n",
    "    for k in Ks:\n",
    "        metrics['NDCG@' + str(k)] = round((100.0 * metrics['NDCG@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['HR@' + str(k)] = round((100.0 * metrics['HR@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['Prec@' + str(k)] = round((100.0 * metrics['Prec@' + str(k)]) / float(total_ndcg), 4)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['CP'] = 0.0\n",
    "    metrics['ZEROS'] = 0.0\n",
    "    metrics['loss'] = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    \n",
    "    NDCG = evaluate_ndcg(model, criterion, reader, hyper_params)\n",
    "    for k in NDCG: metrics[k] = NDCG[k]\n",
    "        \n",
    "    return metrics\n",
    "    \n",
    "#     for x1, x2, y in reader.iter():\n",
    "#         batch += 1\n",
    "#         if is_train_set == True and batch > hyper_params['testing_batch_limit']: break\n",
    "        \n",
    "#         o1, output1 = model(x1)\n",
    "#         o2, output2 = model(x2)\n",
    "#         out_diff = torch.gt(output1, output2).float() - torch.lt(output1, output2).float()\n",
    "\n",
    "#         metrics['loss'] += criterion(o1 + o2, [output1] + [output2], y, x1[1], x2[1], x1[0], x2[0]).data\n",
    "        \n",
    "#         temp_correct  = int(torch.sum((torch.lt(y, 0.0) * torch.lt(out_diff, 0.0)).float()).data)\n",
    "#         temp_correct += int(torch.sum((torch.gt(y, 0.0) * torch.gt(out_diff, 0.0)).float()).data)\n",
    "\n",
    "#         temp_not_correct  = int(torch.sum((torch.lt(y, 0.0) * torch.gt(out_diff, 0.0)).float()).data)\n",
    "#         temp_not_correct += int(torch.sum((torch.gt(y, 0.0) * torch.lt(out_diff, 0.0)).float()).data)\n",
    "\n",
    "#         temp_zeros = int(torch.sum(torch.eq(out_diff, 0.0)).data)\n",
    "\n",
    "#         correct += temp_correct\n",
    "#         not_correct += temp_not_correct\n",
    "#         zeros += temp_zeros\n",
    "#         total += int(y.shape[0])\n",
    "        \n",
    "#         assert temp_correct + temp_not_correct + temp_zeros == int(y.shape[0])\n",
    "\n",
    "#     assert correct + not_correct + zeros == total\n",
    "\n",
    "#     metrics['CP'] = float(correct) / float(total)\n",
    "#     metrics['CP'] *= 100.0\n",
    "#     metrics['CP'] = round(metrics['CP'], 4)\n",
    "\n",
    "#     metrics['ZEROS'] = float(zeros) / float(total)\n",
    "#     metrics['ZEROS'] *= 100.0\n",
    "#     metrics['ZEROS'] = round(metrics['ZEROS'], 4)\n",
    "\n",
    "#     metrics['loss'] = float(metrics['loss'][0]) / float(batch)\n",
    "#     metrics['loss'] = round(metrics['loss'], 4)\n",
    "\n",
    "#     if is_train_set == False:\n",
    "#         ndcg = evaluate_ndcg(model, criterion, reader, hyper_params)\n",
    "#         for k in ndcg: metrics['NDCG@' + str(k)] = ndcg[k]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['user_embed_size'] + hyper_params['item_embed_size'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params, out_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], out_size)\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder_item = Decoder(hyper_params, hyper_params['total_items'])\n",
    "        #self.decoder_user = Decoder(hyper_params, hyper_params['total_users'])\n",
    "        \n",
    "        self._enc_mu = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        self._enc_log_sigma = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self._enc_mu.weight)\n",
    "        nn.init.xavier_normal(self._enc_log_sigma.weight)\n",
    "        \n",
    "        self.user_embed = nn.Embedding(hyper_params['total_users'], hyper_params['user_embed_size'])\n",
    "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
    "        nn.init.normal(self.user_embed.weight.data, mean=0, std=0.01)\n",
    "        nn.init.normal(self.item_embed.weight.data, mean=0, std=0.01)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation_last = nn.Tanh()\n",
    "        if self.hyper_params['loss_type'] == 'bce': self.activation_last = nn.Sigmoid()\n",
    "        \n",
    "        prev = hyper_params['latent_size']\n",
    "        self.layer_hinge1 = nn.Linear(prev, 1)\n",
    "        nn.init.xavier_normal(self.layer_hinge1.weight)\n",
    "        # self.layer_hinge2 = nn.Linear(64, 1)\n",
    "        # nn.init.xavier_normal(self.layer_hinge2.weight)\n",
    "        # xavier_uniform\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        #mu = self.dropout(mu)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        #log_sigma = self.dropout(log_sigma)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float().cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, xuser, xitem):\n",
    "        user = (self.user_embed(xuser))\n",
    "        item = (self.item_embed(xitem))\n",
    "        \n",
    "        send = torch.cat([user, item], dim=-1)\n",
    "#         send = user * item\n",
    "        \n",
    "        h_enc = self.encoder(send)\n",
    "        z = self.sample_latent(h_enc)\n",
    "        dec_item = self.decoder_item(z)\n",
    "        \n",
    "        # dec_user = self.decoder_user(z)\n",
    "        # Can also produce user as decoder output\n",
    "        \n",
    "        output = self.layer_hinge1(z)\n",
    "        #output = self.dropout(output)\n",
    "        # output = self.activation(output)\n",
    "        # output = self.layer_hinge2(output)\n",
    "        # output = self.activation_last(output)\n",
    "                              \n",
    "        return [\n",
    "            dec_item, self.z_mean, self.z_sigma\n",
    "            # self.z_mean, self.z_sigma\n",
    "        ], output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "\n",
    "        self.loss_type = hyper_params['loss_type']\n",
    "        self.m_loss = hyper_params['m_loss']\n",
    "        batch_size = hyper_params['batch_size']\n",
    "\n",
    "        self.zeros_while_max = torch.zeros(int(batch_size)).float()\n",
    "        if is_cuda_available: self.zeros_while_max = self.zeros_while_max.cuda()\n",
    "        self.zeros_while_max = Variable(self.zeros_while_max)\n",
    "        self.exp = Variable(FloatTensor([np.e]))\n",
    "        self.hundred = Variable(FloatTensor([100.0]))\n",
    "        self.cce_movie = nn.CrossEntropyLoss(size_average=True)\n",
    "        #self.cce_user = nn.CrossEntropyLoss(size_average=True)\n",
    "        #self.bce = nn.BCELoss(size_average=True)\n",
    "\n",
    "    def forward(self, o1, o2, tm1, tm2, anneal):\n",
    "        \n",
    "        m1, zm1, zs1, m2, zm2, zs2 = o1\n",
    "        #zm1, zs1, zm2, zs2 = o1\n",
    "        \n",
    "        mean_sq1 = zm1 * zm1\n",
    "        stddev_sq1 = zs1 * zs1\n",
    "        kld  = torch.mean(mean_sq1 + stddev_sq1 - torch.log(stddev_sq1) - 1)\n",
    "        \n",
    "        mean_sq2 = zm2 * zm2\n",
    "        stddev_sq2 = zs2 * zs2\n",
    "        kld += torch.mean(mean_sq2 + stddev_sq2 - torch.log(stddev_sq2) - 1)\n",
    "        \n",
    "        likelihood  = self.cce_movie(m1, tm1)\n",
    "        likelihood += self.cce_movie(m2, tm2)\n",
    "\n",
    "        out_diff = o2[0] - o2[1]\n",
    "        \n",
    "        # Reference: https://papers.nips.cc/paper/3708-ranking-measures-and-loss-functions-in-learning-to-rank.pdf\n",
    "        if self.loss_type == 'hinge':\n",
    "            pairwise_loss = self.m_loss - (out_diff)\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "            \n",
    "        elif self.loss_type == 'bce':\n",
    "            pairwise_loss = self.bce(out_diff, y)\n",
    "            \n",
    "        elif self.loss_type == 'easy_hinge':\n",
    "            # pairwise_loss = torch.log(2.0 - (y * o2))# / np.log(2) # torch.log is base \"e\"\n",
    "            pairwise_loss = torch.log((out_diff*out_diff) - (10*y*out_diff) + 26) - 2\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "            \n",
    "        elif self.loss_type == 'difficult_hinge':\n",
    "            # pairwise_loss = torch.log(2.0 - (y * o2))# / np.log(2) # torch.log is base \"e\"\n",
    "            pairwise_loss = torch.pow(self.hundred, 1 - (y * out_diff)) - 1\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "        \n",
    "        elif self.loss_type == 'saddle':\n",
    "            pairwise_loss = torch.pow(y + out_diff, 2)\n",
    "\n",
    "        elif self.loss_type == 'exp':\n",
    "            pairwise_loss = torch.pow(self.exp, y * out_diff)\n",
    "\n",
    "        elif self.loss_type == 'logistic':\n",
    "            pairwise_loss = torch.mean(torch.log(self.m_loss + torch.pow(self.exp, -(out_diff))))\n",
    "        \n",
    "        final = (0.0 * kld) + (3.8 * pairwise_loss) + (1 * likelihood)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n",
      "\n",
      "\n",
      "Simulation run on: 2018-08-16 04:54:23.770395\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches: 121628\n",
      "Total Users: 75454\n",
      "Total Items: 17647\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (decoder_item): Decoder(\n",
      "    (linear1): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (linear2): Linear(in_features=100, out_features=17647, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (_enc_log_sigma): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (user_embed): Embedding(75454, 128)\n",
      "  (item_embed): Embedding(17647, 128)\n",
      "  (activation): ReLU()\n",
      "  (activation_last): Tanh()\n",
      "  (layer_hinge1): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n",
      "| epoch   1 |  2000/121628 batches | ms/batch 13.96 | loss 19.4710\n",
      "| epoch   1 |  4000/121628 batches | ms/batch 13.88 | loss 18.7465\n",
      "| epoch   1 |  6000/121628 batches | ms/batch 13.88 | loss 18.7746\n",
      "| epoch   1 |  8000/121628 batches | ms/batch 13.89 | loss 18.6498\n",
      "| epoch   1 | 10000/121628 batches | ms/batch 13.88 | loss 18.7094\n",
      "| epoch   1 | 12000/121628 batches | ms/batch 13.97 | loss 18.7026\n",
      "| epoch   1 | 14000/121628 batches | ms/batch 14.04 | loss 18.7323\n",
      "| epoch   1 | 16000/121628 batches | ms/batch 14.04 | loss 18.6872\n",
      "| epoch   1 | 18000/121628 batches | ms/batch 14.04 | loss 18.7047\n",
      "| epoch   1 | 20000/121628 batches | ms/batch 14.04 | loss 18.6705\n",
      "| epoch   1 | 22000/121628 batches | ms/batch 14.03 | loss 18.6915\n",
      "| epoch   1 | 24000/121628 batches | ms/batch 14.04 | loss 18.7072\n",
      "| epoch   1 | 26000/121628 batches | ms/batch 14.03 | loss 18.7423\n",
      "| epoch   1 | 28000/121628 batches | ms/batch 14.01 | loss 18.6298\n",
      "| epoch   1 | 30000/121628 batches | ms/batch 14.01 | loss 18.6914\n",
      "| epoch   1 | 32000/121628 batches | ms/batch 14.01 | loss 18.6753\n",
      "| epoch   1 | 34000/121628 batches | ms/batch 14.01 | loss 18.7002\n",
      "| epoch   1 | 36000/121628 batches | ms/batch 14.01 | loss 18.7212\n",
      "| epoch   1 | 38000/121628 batches | ms/batch 14.01 | loss 18.6680\n",
      "| epoch   1 | 40000/121628 batches | ms/batch 14.02 | loss 18.7381\n",
      "| epoch   1 | 42000/121628 batches | ms/batch 14.04 | loss 18.6321\n",
      "| epoch   1 | 44000/121628 batches | ms/batch 14.04 | loss 18.6477\n",
      "| epoch   1 | 46000/121628 batches | ms/batch 14.05 | loss 18.6568\n",
      "| epoch   1 | 48000/121628 batches | ms/batch 14.04 | loss 18.6206\n",
      "| epoch   1 | 50000/121628 batches | ms/batch 14.03 | loss 18.6492\n",
      "| epoch   1 | 52000/121628 batches | ms/batch 14.03 | loss 18.5994\n",
      "| epoch   1 | 54000/121628 batches | ms/batch 14.05 | loss 18.5910\n",
      "| epoch   1 | 56000/121628 batches | ms/batch 14.04 | loss 18.5958\n",
      "| epoch   1 | 58000/121628 batches | ms/batch 13.95 | loss 18.6410\n",
      "| epoch   1 | 60000/121628 batches | ms/batch 13.87 | loss 18.6369\n",
      "| epoch   1 | 62000/121628 batches | ms/batch 13.86 | loss 18.5656\n",
      "| epoch   1 | 64000/121628 batches | ms/batch 13.94 | loss 18.5782\n",
      "| epoch   1 | 66000/121628 batches | ms/batch 14.06 | loss 18.6371\n",
      "| epoch   1 | 68000/121628 batches | ms/batch 14.00 | loss 18.6070\n",
      "| epoch   1 | 70000/121628 batches | ms/batch 13.88 | loss 18.5541\n",
      "| epoch   1 | 72000/121628 batches | ms/batch 14.06 | loss 18.5460\n",
      "| epoch   1 | 74000/121628 batches | ms/batch 14.04 | loss 18.6126\n",
      "| epoch   1 | 76000/121628 batches | ms/batch 14.04 | loss 18.6366\n",
      "| epoch   1 | 78000/121628 batches | ms/batch 14.04 | loss 18.5817\n",
      "| epoch   1 | 80000/121628 batches | ms/batch 14.04 | loss 18.5509\n",
      "| epoch   1 | 82000/121628 batches | ms/batch 14.04 | loss 18.5482\n",
      "| epoch   1 | 84000/121628 batches | ms/batch 14.04 | loss 18.5255\n",
      "| epoch   1 | 86000/121628 batches | ms/batch 14.05 | loss 18.5970\n",
      "| epoch   1 | 88000/121628 batches | ms/batch 14.05 | loss 18.5331\n",
      "| epoch   1 | 90000/121628 batches | ms/batch 14.04 | loss 18.5124\n",
      "| epoch   1 | 92000/121628 batches | ms/batch 14.04 | loss 18.5656\n",
      "| epoch   1 | 94000/121628 batches | ms/batch 14.04 | loss 18.5707\n",
      "| epoch   1 | 96000/121628 batches | ms/batch 14.04 | loss 18.5527\n",
      "| epoch   1 | 98000/121628 batches | ms/batch 14.04 | loss 18.5627\n",
      "| epoch   1 | 100000/121628 batches | ms/batch 14.04 | loss 18.5192\n",
      "| epoch   1 | 102000/121628 batches | ms/batch 14.06 | loss 18.5009\n",
      "| epoch   1 | 104000/121628 batches | ms/batch 14.04 | loss 18.5167\n",
      "| epoch   1 | 106000/121628 batches | ms/batch 14.04 | loss 18.5717\n",
      "| epoch   1 | 108000/121628 batches | ms/batch 14.04 | loss 18.5457\n",
      "| epoch   1 | 110000/121628 batches | ms/batch 14.05 | loss 18.5066\n",
      "| epoch   1 | 112000/121628 batches | ms/batch 14.03 | loss 18.3985\n",
      "| epoch   1 | 114000/121628 batches | ms/batch 14.03 | loss 18.3567\n",
      "| epoch   1 | 116000/121628 batches | ms/batch 14.03 | loss 18.3457\n",
      "| epoch   1 | 118000/121628 batches | ms/batch 14.03 | loss 18.3785\n",
      "| epoch   1 | 120000/121628 batches | ms/batch 14.05 | loss 18.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8000 [00:00<08:35, 15.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 121628/121628 batches | ms/batch 14.04 | loss 18.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [08:08<00:00, 16.37it/s]\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2192.89s | CP = 0.0 | ZEROS = 0.0 | loss = 0.0 | NDCG@10 = 4.1407 | HR@10 = 1.3056 | Prec@10 = 3.86 | NDCG@100 = 6.0774 | HR@100 = 9.2612 | Prec@100 = 2.8833 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  2000/121628 batches | ms/batch 14.03 | loss 18.2641\n",
      "| epoch   2 |  4000/121628 batches | ms/batch 14.02 | loss 18.3125\n",
      "| epoch   2 |  6000/121628 batches | ms/batch 14.02 | loss 18.4493\n",
      "| epoch   2 |  8000/121628 batches | ms/batch 14.03 | loss 18.3800\n",
      "| epoch   2 | 10000/121628 batches | ms/batch 14.04 | loss 18.4609\n",
      "| epoch   2 | 12000/121628 batches | ms/batch 14.05 | loss 18.4579\n",
      "| epoch   2 | 14000/121628 batches | ms/batch 14.05 | loss 18.5111\n",
      "| epoch   2 | 16000/121628 batches | ms/batch 14.05 | loss 18.5030\n",
      "| epoch   2 | 18000/121628 batches | ms/batch 14.05 | loss 18.4793\n",
      "| epoch   2 | 20000/121628 batches | ms/batch 14.08 | loss 18.4323\n",
      "| epoch   2 | 22000/121628 batches | ms/batch 14.08 | loss 18.4424\n",
      "| epoch   2 | 24000/121628 batches | ms/batch 14.11 | loss 18.4665\n",
      "| epoch   2 | 26000/121628 batches | ms/batch 14.10 | loss 18.4858\n",
      "| epoch   2 | 28000/121628 batches | ms/batch 14.08 | loss 18.3980\n",
      "| epoch   2 | 30000/121628 batches | ms/batch 14.07 | loss 18.4514\n",
      "| epoch   2 | 32000/121628 batches | ms/batch 14.05 | loss 18.4128\n",
      "| epoch   2 | 34000/121628 batches | ms/batch 14.05 | loss 18.4381\n",
      "| epoch   2 | 36000/121628 batches | ms/batch 14.05 | loss 18.4609\n",
      "| epoch   2 | 38000/121628 batches | ms/batch 14.05 | loss 18.3866\n",
      "| epoch   2 | 40000/121628 batches | ms/batch 14.05 | loss 18.4773\n",
      "| epoch   2 | 42000/121628 batches | ms/batch 14.05 | loss 18.3710\n",
      "| epoch   2 | 44000/121628 batches | ms/batch 14.05 | loss 18.3748\n",
      "| epoch   2 | 46000/121628 batches | ms/batch 14.05 | loss 18.3943\n",
      "| epoch   2 | 48000/121628 batches | ms/batch 14.07 | loss 18.3851\n",
      "| epoch   2 | 50000/121628 batches | ms/batch 14.05 | loss 18.4263\n",
      "| epoch   2 | 52000/121628 batches | ms/batch 14.06 | loss 18.3723\n",
      "| epoch   2 | 54000/121628 batches | ms/batch 14.05 | loss 18.3644\n",
      "| epoch   2 | 56000/121628 batches | ms/batch 14.06 | loss 18.3776\n",
      "| epoch   2 | 58000/121628 batches | ms/batch 14.05 | loss 18.4269\n",
      "| epoch   2 | 60000/121628 batches | ms/batch 14.05 | loss 18.4219\n",
      "| epoch   2 | 62000/121628 batches | ms/batch 14.05 | loss 18.3612\n",
      "| epoch   2 | 64000/121628 batches | ms/batch 14.05 | loss 18.3893\n",
      "| epoch   2 | 66000/121628 batches | ms/batch 14.05 | loss 18.4152\n",
      "| epoch   2 | 68000/121628 batches | ms/batch 14.07 | loss 18.3914\n",
      "| epoch   2 | 70000/121628 batches | ms/batch 14.06 | loss 18.3650\n",
      "| epoch   2 | 72000/121628 batches | ms/batch 14.05 | loss 18.3684\n",
      "| epoch   2 | 74000/121628 batches | ms/batch 14.06 | loss 18.4499\n",
      "| epoch   2 | 76000/121628 batches | ms/batch 14.06 | loss 18.4634\n",
      "| epoch   2 | 78000/121628 batches | ms/batch 14.05 | loss 18.4105\n",
      "| epoch   2 | 80000/121628 batches | ms/batch 14.07 | loss 18.3805\n",
      "| epoch   2 | 82000/121628 batches | ms/batch 14.05 | loss 18.3578\n",
      "| epoch   2 | 84000/121628 batches | ms/batch 14.07 | loss 18.3635\n",
      "| epoch   2 | 86000/121628 batches | ms/batch 14.06 | loss 18.4016\n",
      "| epoch   2 | 88000/121628 batches | ms/batch 14.05 | loss 18.3703\n",
      "| epoch   2 | 90000/121628 batches | ms/batch 14.05 | loss 18.3236\n",
      "| epoch   2 | 92000/121628 batches | ms/batch 14.07 | loss 18.4123\n",
      "| epoch   2 | 94000/121628 batches | ms/batch 14.05 | loss 18.4036\n",
      "| epoch   2 | 96000/121628 batches | ms/batch 14.06 | loss 18.3726\n",
      "| epoch   2 | 98000/121628 batches | ms/batch 14.10 | loss 18.3700\n",
      "| epoch   2 | 100000/121628 batches | ms/batch 14.07 | loss 18.3358\n",
      "| epoch   2 | 102000/121628 batches | ms/batch 14.09 | loss 18.3556\n",
      "| epoch   2 | 104000/121628 batches | ms/batch 14.10 | loss 18.3242\n",
      "| epoch   2 | 106000/121628 batches | ms/batch 14.06 | loss 18.3746\n",
      "| epoch   2 | 108000/121628 batches | ms/batch 14.06 | loss 18.3713\n",
      "| epoch   2 | 110000/121628 batches | ms/batch 14.08 | loss 18.3275\n",
      "| epoch   2 | 112000/121628 batches | ms/batch 14.05 | loss 18.2121\n",
      "| epoch   2 | 114000/121628 batches | ms/batch 14.04 | loss 18.2010\n",
      "| epoch   2 | 116000/121628 batches | ms/batch 14.04 | loss 18.1854\n",
      "| epoch   2 | 118000/121628 batches | ms/batch 14.04 | loss 18.2164\n",
      "| epoch   2 | 120000/121628 batches | ms/batch 14.04 | loss 18.2072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8000 [00:00<08:31, 15.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 121628/121628 batches | ms/batch 14.05 | loss 18.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [08:08<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 2197.99s | CP = 0.0 | ZEROS = 0.0 | loss = 0.0 | NDCG@10 = 3.7882 | HR@10 = 1.2456 | Prec@10 = 3.55 | NDCG@100 = 6.0211 | HR@100 = 9.5141 | Prec@100 = 2.9195 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  2000/121628 batches | ms/batch 14.18 | loss 18.1229\n",
      "| epoch   3 |  4000/121628 batches | ms/batch 14.47 | loss 18.1593\n",
      "| epoch   3 |  6000/121628 batches | ms/batch 15.09 | loss 18.2952\n",
      "| epoch   3 |  8000/121628 batches | ms/batch 14.12 | loss 18.2137\n",
      "| epoch   3 | 10000/121628 batches | ms/batch 14.05 | loss 18.3181\n",
      "| epoch   3 | 12000/121628 batches | ms/batch 14.06 | loss 18.3014\n",
      "| epoch   3 | 14000/121628 batches | ms/batch 14.07 | loss 18.3350\n",
      "| epoch   3 | 16000/121628 batches | ms/batch 14.07 | loss 18.3301\n",
      "| epoch   3 | 18000/121628 batches | ms/batch 13.98 | loss 18.3054\n",
      "| epoch   3 | 20000/121628 batches | ms/batch 13.97 | loss 18.2445\n",
      "| epoch   3 | 22000/121628 batches | ms/batch 13.97 | loss 18.2850\n",
      "| epoch   3 | 24000/121628 batches | ms/batch 13.97 | loss 18.3183\n",
      "| epoch   3 | 26000/121628 batches | ms/batch 13.97 | loss 18.3211\n",
      "| epoch   3 | 28000/121628 batches | ms/batch 13.97 | loss 18.2257\n",
      "| epoch   3 | 30000/121628 batches | ms/batch 13.97 | loss 18.3113\n",
      "| epoch   3 | 32000/121628 batches | ms/batch 13.97 | loss 18.2744\n",
      "| epoch   3 | 34000/121628 batches | ms/batch 13.97 | loss 18.3076\n",
      "| epoch   3 | 36000/121628 batches | ms/batch 13.97 | loss 18.3209\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type double without overflow: inf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-34f750edaed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Calulating the metrics on the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-34f750edaed4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(reader)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtemp_o3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_o4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxuser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_o1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp_o3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtemp_o2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_o4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# loss = criterion(temp_o1 + temp_o3, [temp_o2, temp_o4], anneal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/noveen/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1930cc81e65a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, o1, o2, tm1, tm2, anneal)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'logistic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mpairwise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpairwise_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type double without overflow: inf"
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "\n",
    "    for xuser, xpos, xneg in reader.iter():\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        temp_o1, temp_o2 = model(xuser, xpos)\n",
    "        temp_o3, temp_o4 = model(xuser, xneg)\n",
    "        \n",
    "        loss = criterion(temp_o1 + temp_o3, [temp_o2, temp_o4], xpos, xneg, anneal)\n",
    "        # loss = criterion(temp_o1 + temp_o3, [temp_o2, temp_o4], anneal)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_users, total_items = load_data(hyper_params)\n",
    "hyper_params['total_users'] = total_users\n",
    "hyper_params['total_items'] = total_items\n",
    "# hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "anneal = 0.1\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "# file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "#         metrics = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "#         string = \"\"\n",
    "#         for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "#         string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "#         ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "#         ss += string\n",
    "#         ss += '\\n'\n",
    "#         ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        anneal += 0.1\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
