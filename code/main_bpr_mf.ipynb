{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NOTES:\n",
    "\n",
    "- Try having two different layers for mu and sigma\n",
    "- Never using dropout\n",
    "- Not using L2 Norm at input\n",
    "\n",
    "'''\n",
    "\n",
    "hyper_params = {\n",
    "    'data_base': 'saved_data/pro_sg/',\n",
    "    'project_name': 'bpr_mf',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'history_split_test': [0.8, 0.2], # Part of test history to train on : Part of test history to test\n",
    "\n",
    "    'learning_rate': 0.01, # learning rate is required only if optimizer is adagrad\n",
    "    'optimizer': 'adam',\n",
    "    'weight_decay': float(1e-2),\n",
    "\n",
    "    'epochs': 50,\n",
    "    'batch_size': 1,\n",
    "    \n",
    "    'item_embed_size': 256,\n",
    "    'rnn_size': 200,\n",
    "    'hidden_size': 150,\n",
    "    'latent_size': 64,\n",
    "    'loss_type': 'predict_next', # [predict_next, same, prefix, postfix, exp_decay, next_k]\n",
    "    'next_k': 5,\n",
    "\n",
    "    'number_users_to_keep': 10000000000000,\n",
    "    'batch_log_interval': 1000,\n",
    "    'train_cp_users': 200,\n",
    "    'exploding_clip': 0.25,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "file_name += '_loss_type_' + str(hyper_params['loss_type'])\n",
    "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
    "file_name += '_rnn_size_' + str(hyper_params['rnn_size'])\n",
    "file_name += '_latent_size_' + str(hyper_params['latent_size'])\n",
    "\n",
    "hyper_params['log_file'] = 'saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = 'saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    \n",
    "    train = pd.read_csv(hyper_params['data_base'] + 'train.csv')\n",
    "    train.columns = ['user', 'movie', 'rating']\n",
    "    \n",
    "    test_train = pd.read_csv(hyper_params['data_base'] + 'test_tr.csv')\n",
    "    test_train.columns = ['user', 'movie', 'rating']\n",
    "    \n",
    "    test_test = pd.read_csv(hyper_params['data_base'] + 'test_te.csv')\n",
    "    test_test.columns = ['user', 'movie', 'rating']\n",
    "    \n",
    "    # Starting test users' id from 0\n",
    "    test_train['user'] = test_train['user'] - min(test_train['user'])\n",
    "    test_test['user'] = test_test['user'] - min(test_test['user'])\n",
    "    \n",
    "    unique_sid = list()\n",
    "    with open(hyper_params['data_base'] + 'unique_sid.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "    num_items = len(unique_sid)\n",
    "    \n",
    "    num_users_train = max(train['user']) + 1\n",
    "    num_users_test = max(test_train['user']) + 1\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "\n",
    "    train_reader = DataReader(hyper_params, train, None, num_users_train, num_items, True)\n",
    "    test_reader = DataReader(hyper_params, test_train, test_test, num_users_test, num_items, False)\n",
    "\n",
    "    return train_reader, test_reader, num_items\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, hyper_params, data_train, data_test, num_users, num_items, is_training):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.data_train = data_train\n",
    "        self.data_test = data_test\n",
    "        self.is_training = is_training\n",
    "        self.all_users = []\n",
    "        \n",
    "        self.prep()\n",
    "        self.number()\n",
    "\n",
    "    def prep(self):\n",
    "        self.data = []\n",
    "        for i in range(self.num_users): self.data.append([])\n",
    "        for index, row in self.data_train.iterrows():\n",
    "            self.data[row['user']].append([ int(row['movie']), int(row['rating']) ])\n",
    "        \n",
    "        if self.is_training == False:\n",
    "            self.data_te = []\n",
    "            for i in range(self.num_users): self.data_te.append([])\n",
    "            for index, row in self.data_test.iterrows():\n",
    "                self.data_te[row['user']].append([ int(row['movie']), int(row['rating']) ])\n",
    "        \n",
    "    def number(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "        y_batch = []\n",
    "\n",
    "        for user in range(len(self.data)):\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            y_batch.append(0)\n",
    "\n",
    "            if len(y_batch) == self.batch_size:\n",
    "                y_batch = []\n",
    "                count += 1\n",
    "\n",
    "        self.num_b = count\n",
    "        \n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        y_batch = []\n",
    "        now_at = 0\n",
    "        \n",
    "        user_iterate_order = list(range(len(self.data)))\n",
    "        # np.random.shuffle(user_iterate_order)\n",
    "        \n",
    "        for user in user_iterate_order:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "            \n",
    "            if self.hyper_params['loss_type'] == 'predict_next':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ self.data[user][1] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][1:] ])\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'next_k':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in self.data[user][1:1+self.hyper_params['next_k']] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][1:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'same':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ self.data[user][0] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'prefix':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ self.data[user][0] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][1:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'postfix':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in self.data[user][:] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'exp_decay':\n",
    "                x_batch.append([ i[0] for i in self.data[user][:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    # 0, LongTensor([ i[0] for i in self.data[user][:] ]), FloatTensor([ np.e ** (-1.0 * i) for i in range(len(self.data[user][:])) ]), \n",
    "                    0, LongTensor([ i[0] for i in self.data[user][:] ]), FloatTensor([ 1.0 / (i + 1.0) for i in range(len(self.data[user][:])) ]), \n",
    "                    # 0, LongTensor([ i[0] for i in self.data[user][:] ]), FloatTensor([ 1.0 / np.log2(i + 2.0) for i in range(len(self.data[user][:])) ]), \n",
    "                )\n",
    "                y_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
    "            \n",
    "            now_at += 1\n",
    "    \n",
    "            if now_at == self.batch_size:\n",
    "\n",
    "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False), y_batch\n",
    "\n",
    "                x_batch = []\n",
    "                y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                y_batch = []\n",
    "                now_at = 0\n",
    "\n",
    "    def iter_eval(self):\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "        y_batch = []\n",
    "        test_movies, test_movies_r = [], []\n",
    "        now_at = 0\n",
    "        \n",
    "        for user in range(len(self.data)):\n",
    "            \n",
    "            if self.is_training == True: \n",
    "                base_predictions_on = self.data[user][:int(0.8 * len(self.data[user]))]\n",
    "                heldout_movies = self.data[user][int(0.8 * len(self.data[user])):]\n",
    "            else:\n",
    "                base_predictions_on = self.data[user]\n",
    "                heldout_movies = self.data_te[user]\n",
    "                \n",
    "            if self.hyper_params['loss_type'] == 'predict_next':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ base_predictions_on[1] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[1:] ])\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'next_k':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in base_predictions_on[1:1+self.hyper_params['next_k']] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[1:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'same':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ base_predictions_on[0] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'prefix':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in [ base_predictions_on[0] ] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[1:] ])\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'postfix':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    0, LongTensor([ i[0] for i in base_predictions_on[:] ]), 1.0\n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'exp_decay':\n",
    "                x_batch.append([ i[0] for i in base_predictions_on[:] ])\n",
    "                y_batch_s[now_at, :].scatter_(\n",
    "                    # 0, LongTensor([ i[0] for i in base_predictions_on[:] ]), FloatTensor([ np.e ** (-1.0 * i) for i in range(len(base_predictions_on[:])) ]), \n",
    "                    0, LongTensor([ i[0] for i in base_predictions_on[:] ]), FloatTensor([ 1.0 / (i + 1.0) for i in range(len(base_predictions_on[:])) ]), \n",
    "                    # 0, LongTensor([ i[0] for i in base_predictions_on[:] ]), FloatTensor([ 1.0 / np.log2(i + 2.0) for i in range(len(base_predictions_on[:])) ]), \n",
    "                )\n",
    "                y_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
    "            \n",
    "            now_at += 1\n",
    "            \n",
    "            test_movies.append([ i[0] for i in heldout_movies ])\n",
    "            test_movies_r.append([ i[1] for i in heldout_movies ])\n",
    "            \n",
    "            if now_at == self.batch_size:\n",
    "                \n",
    "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False), \\\n",
    "                y_batch, test_movies, test_movies_r\n",
    "                \n",
    "                x_batch = []\n",
    "                y_batch_s = torch.zeros(self.batch_size, self.num_items).cuda()\n",
    "                y_batch = []\n",
    "                test_movies, test_movies_r = [], []\n",
    "                now_at = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['loss'] = 0.0\n",
    "    Ks = [100]\n",
    "    for k in Ks: \n",
    "        metrics['NDCG@' + str(k)] = 0.0\n",
    "        metrics['HR@' + str(k)] = 0.0\n",
    "        metrics['Prec@' + str(k)] = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    total_ndcg = 0.0\n",
    "\n",
    "    for x, y_s, y, test_movies, test_movies_r in reader.iter_eval():\n",
    "        batch += 1\n",
    "        if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
    "\n",
    "        decoder_output, z_mean, z_log_sigma = model(x)\n",
    "        \n",
    "        metrics['loss'] += criterion(decoder_output, z_mean, z_log_sigma, y_s, y, 0.2).data[0]\n",
    "        \n",
    "        # decoder_output[X.nonzero()] = -np.inf\n",
    "        decoder_output = decoder_output.data\n",
    "        \n",
    "        x_scattered = torch.zeros(decoder_output.shape[0], decoder_output.shape[2]).cuda()\n",
    "        x_scattered[0, :].scatter_(0, x[0].data, 1.0)\n",
    "        # If loss type is predict next, the last element in the train sequence is not included in x\n",
    "        if hyper_params['loss_type'] == 'predict_next': x_scattered[0, y[0][-1]] = 1.0\n",
    "        \n",
    "        last_predictions = decoder_output[:, -1, :] - \\\n",
    "        (torch.abs(decoder_output[:, -1, :] * x_scattered) * 100000000)\n",
    "        \n",
    "        for batch_num in range(last_predictions.shape[0]):\n",
    "            predicted_scores = last_predictions[batch_num]\n",
    "            actual_movies_watched = test_movies[batch_num]\n",
    "            actual_movies_ratings = test_movies_r[batch_num]\n",
    "                    \n",
    "            # Calculate NDCG\n",
    "            _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
    "            for k in Ks:\n",
    "                best = 0.0\n",
    "                now_at = 0.0\n",
    "                dcg = 0.0\n",
    "                hr = 0.0\n",
    "                \n",
    "                rec_list = list(argsorted[:k].cpu().numpy())\n",
    "                for m in range(len(actual_movies_watched)):\n",
    "                    movie = actual_movies_watched[m]\n",
    "                    now_at += 1.0\n",
    "                    if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
    "                    \n",
    "                    if movie not in rec_list: continue\n",
    "                    hr += 1.0\n",
    "                    dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
    "                \n",
    "                metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
    "                metrics['HR@' + str(k)] += float(hr) / float(len(actual_movies_watched))\n",
    "                metrics['Prec@' + str(k)] += float(hr) / float(k)\n",
    "                \n",
    "            total_ndcg += 1.0\n",
    "    \n",
    "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
    "    metrics['loss'] = round(metrics['loss'], 4)\n",
    "    \n",
    "    for k in Ks:\n",
    "        metrics['NDCG@' + str(k)] = round((100.0 * metrics['NDCG@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['HR@' + str(k)] = round((100.0 * metrics['HR@' + str(k)]) / float(total_ndcg), 4)\n",
    "        metrics['Prec@' + str(k)] = round((100.0 * metrics['Prec@' + str(k)]) / float(total_ndcg), 4)\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['rnn_size'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.Tanh()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder = Decoder(hyper_params)\n",
    "        \n",
    "        # No +1 means can never pad, hence bsz has to be equal 1\n",
    "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            hyper_params['item_embed_size'], hyper_params['rnn_size'], \n",
    "            batch_first=True, num_layers=1\n",
    "        )\n",
    "        \n",
    "        self.layer_temp = nn.Linear(hyper_params['hidden_size'], 2 * hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self.layer_temp.weight)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        temp_out = self.layer_temp(h_enc)\n",
    "        \n",
    "        mu = temp_out[:, :self.hyper_params['latent_size']]\n",
    "        log_sigma = temp_out[:, self.hyper_params['latent_size']:]\n",
    "        \n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "        if is_cuda_available: std_z = std_z.cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_log_sigma = log_sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = x[0]\n",
    "        neg = x[1]\n",
    "        \n",
    "        pos = self.item_embed(pos)\n",
    "        neg = self.item_embed(neg)\n",
    "        \n",
    "                              \n",
    "        return dec_out, self.z_mean, self.z_log_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def forward(self, decoder_output, mu_q, logvar_q, y_true_s, y_batch, anneal):\n",
    "        # Calculate KL Divergence loss\n",
    "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
    "    \n",
    "        # decoder_output shape : [batch_size, seq_len, all_items]\n",
    "        out_shape = decoder_output.shape\n",
    "        decoder_output = F.log_softmax(decoder_output, -1)\n",
    "        likelihood = None\n",
    "        num_ones = torch.sum(y_true_s, -1)\n",
    "        \n",
    "        for time_step in range(out_shape[1]):\n",
    "            \n",
    "            # Calculate Likelihood\n",
    "            if likelihood is None: \n",
    "                likelihood = torch.mean(\n",
    "                    -1.0 * torch.sum(y_true_s * decoder_output[:, time_step, :], -1) / num_ones\n",
    "                )\n",
    "            else:\n",
    "                likelihood = likelihood + torch.mean(\n",
    "                    -1.0 * torch.sum(y_true_s * decoder_output[:, time_step, :], -1) / num_ones\n",
    "                )\n",
    "            \n",
    "            # Update y_true_s\n",
    "            y_true_clone = y_true_s.clone()\n",
    "            if self.hyper_params['loss_type'] == 'predict_next' and time_step != out_shape[1] - 1:\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 0.0\n",
    "                y_true_clone[0, y_batch[0][time_step+1]] = 1.0\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'next_k' and time_step <= out_shape[1] - 1:\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 0.0\n",
    "                if time_step + 1 + self.hyper_params['next_k'] <= out_shape[1] - 1:\n",
    "                    y_true_clone[0, y_batch[0][time_step + 1 + self.hyper_params['next_k']]] = 1.0\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'same' and time_step != out_shape[1] - 1:\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 0.0\n",
    "                y_true_clone[0, y_batch[0][time_step+1]] = 1.0\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'prefix' and time_step != out_shape[1] - 1:\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 1.0\n",
    "                num_ones = num_ones + 1.0\n",
    "            \n",
    "            elif self.hyper_params['loss_type'] == 'postfix' and time_step != out_shape[1] - 1:\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 0.0\n",
    "                num_ones = num_ones - 1.0\n",
    "                \n",
    "            elif self.hyper_params['loss_type'] == 'exp_decay' and time_step != out_shape[1] - 1:\n",
    "                num_ones = num_ones - y_true_clone[0, y_batch[0][time_step]]\n",
    "                y_true_clone[0, y_batch[0][time_step]] = 0.0\n",
    "            y_true_s = y_true_clone\n",
    "            \n",
    "        # likelihood = 100.0 * (likelihood / float(out_shape[1]))\n",
    "        \n",
    "        final = (anneal * kld) + (1.0 * likelihood)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n",
      "\n",
      "\n",
      "Simulation run on: 2018-08-04 19:04:00.969956\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches: 4034\n",
      "Number of test batches: 1000\n",
      "Total Items: 3468\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=200, out_features=150, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear1): Linear(in_features=64, out_features=150, bias=True)\n",
      "    (linear2): Linear(in_features=150, out_features=3468, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (item_embed): Embedding(3468, 256)\n",
      "  (gru): GRU(256, 200, batch_first=True)\n",
      "  (layer_temp): Linear(in_features=150, out_features=128, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n",
      "| epoch   1 |  1000/ 4034 batches | ms/batch 35.16 | loss 651.6720\n",
      "| epoch   1 |  2000/ 4034 batches | ms/batch 36.18 | loss 627.0700\n",
      "| epoch   1 |  3000/ 4034 batches | ms/batch 35.49 | loss 588.7292\n",
      "| epoch   1 |  4000/ 4034 batches | ms/batch 39.33 | loss 635.1347\n",
      "| epoch   1 |  4034/ 4034 batches | ms/batch 27.47 | loss 429.2705\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 166.29s | loss = 420.6608 | NDCG@100 = 25.2154 | HR@100 = 42.3858 | Prec@100 = 5.925 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 166.29s | loss = 462.6852 | NDCG@100 = 25.7037 | HR@100 = 42.9905 | Prec@100 = 6.028 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 4034 batches | ms/batch 35.14 | loss 551.4282\n",
      "| epoch   2 |  2000/ 4034 batches | ms/batch 36.32 | loss 569.7032\n",
      "| epoch   2 |  3000/ 4034 batches | ms/batch 35.50 | loss 556.4690\n",
      "| epoch   2 |  4000/ 4034 batches | ms/batch 39.26 | loss 608.4289\n",
      "| epoch   2 |  4034/ 4034 batches | ms/batch 27.24 | loss 412.8858\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 166.28s | loss = 408.43 | NDCG@100 = 27.7336 | HR@100 = 45.9627 | Prec@100 = 6.445 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 166.28s | loss = 451.1329 | NDCG@100 = 27.2253 | HR@100 = 45.0364 | Prec@100 = 6.303 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  1000/ 4034 batches | ms/batch 34.98 | loss 533.0946\n",
      "| epoch   3 |  2000/ 4034 batches | ms/batch 35.62 | loss 553.4946\n",
      "| epoch   3 |  3000/ 4034 batches | ms/batch 34.98 | loss 543.7872\n",
      "| epoch   3 |  4000/ 4034 batches | ms/batch 38.24 | loss 596.4447\n",
      "| epoch   3 |  4034/ 4034 batches | ms/batch 26.85 | loss 404.4433\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 163.43s | loss = 403.709 | NDCG@100 = 28.5128 | HR@100 = 46.986 | Prec@100 = 6.545 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 163.43s | loss = 447.2337 | NDCG@100 = 27.9343 | HR@100 = 45.9626 | Prec@100 = 6.433 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  1000/ 4034 batches | ms/batch 34.26 | loss 523.9437\n",
      "| epoch   4 |  2000/ 4034 batches | ms/batch 33.80 | loss 545.1379\n",
      "| epoch   4 |  3000/ 4034 batches | ms/batch 34.13 | loss 536.5057\n",
      "| epoch   4 |  4000/ 4034 batches | ms/batch 39.15 | loss 589.4008\n",
      "| epoch   4 |  4034/ 4034 batches | ms/batch 27.13 | loss 399.9919\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 161.46s | loss = 401.6877 | NDCG@100 = 28.6988 | HR@100 = 46.8634 | Prec@100 = 6.565 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 161.46s | loss = 446.2713 | NDCG@100 = 28.4046 | HR@100 = 46.773 | Prec@100 = 6.459 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |  1000/ 4034 batches | ms/batch 35.06 | loss 518.1961\n",
      "| epoch   5 |  2000/ 4034 batches | ms/batch 36.51 | loss 539.5271\n",
      "| epoch   5 |  3000/ 4034 batches | ms/batch 35.58 | loss 531.5783\n",
      "| epoch   5 |  4000/ 4034 batches | ms/batch 39.37 | loss 584.3987\n",
      "| epoch   5 |  4034/ 4034 batches | ms/batch 27.39 | loss 396.5453\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 166.66s | loss = 398.8411 | NDCG@100 = 29.4129 | HR@100 = 47.5055 | Prec@100 = 6.65 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 166.66s | loss = 444.6859 | NDCG@100 = 28.5143 | HR@100 = 47.1231 | Prec@100 = 6.487 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |  1000/ 4034 batches | ms/batch 34.84 | loss 513.9482\n",
      "| epoch   6 |  2000/ 4034 batches | ms/batch 36.20 | loss 535.2419\n",
      "| epoch   6 |  3000/ 4034 batches | ms/batch 35.36 | loss 527.8483\n",
      "| epoch   6 |  4000/ 4034 batches | ms/batch 38.97 | loss 580.4392\n",
      "| epoch   6 |  4034/ 4034 batches | ms/batch 26.95 | loss 394.0252\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 165.17s | loss = 397.8482 | NDCG@100 = 29.1169 | HR@100 = 47.1792 | Prec@100 = 6.605 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 165.17s | loss = 444.6866 | NDCG@100 = 28.6113 | HR@100 = 47.2794 | Prec@100 = 6.5 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |  1000/ 4034 batches | ms/batch 34.48 | loss 510.5465\n",
      "| epoch   7 |  2000/ 4034 batches | ms/batch 36.11 | loss 531.7948\n",
      "| epoch   7 |  3000/ 4034 batches | ms/batch 35.22 | loss 524.8224\n",
      "| epoch   7 |  4000/ 4034 batches | ms/batch 38.22 | loss 577.1304\n",
      "| epoch   7 |  4034/ 4034 batches | ms/batch 26.76 | loss 391.4916\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 163.49s | loss = 396.7065 | NDCG@100 = 29.5581 | HR@100 = 47.4045 | Prec@100 = 6.65 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 163.49s | loss = 444.3982 | NDCG@100 = 28.6076 | HR@100 = 47.3728 | Prec@100 = 6.495 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |  1000/ 4034 batches | ms/batch 34.07 | loss 507.9445\n",
      "| epoch   8 |  2000/ 4034 batches | ms/batch 35.55 | loss 529.0999\n",
      "| epoch   8 |  3000/ 4034 batches | ms/batch 34.72 | loss 522.5933\n",
      "| epoch   8 |  4000/ 4034 batches | ms/batch 38.89 | loss 574.5224\n",
      "| epoch   8 |  4034/ 4034 batches | ms/batch 26.71 | loss 389.2898\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 163.08s | loss = 395.6255 | NDCG@100 = 29.4824 | HR@100 = 48.1844 | Prec@100 = 6.695 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 163.08s | loss = 444.3615 | NDCG@100 = 28.7748 | HR@100 = 47.3713 | Prec@100 = 6.487 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |  1000/ 4034 batches | ms/batch 34.30 | loss 505.7551\n",
      "| epoch   9 |  2000/ 4034 batches | ms/batch 35.54 | loss 526.6854\n",
      "| epoch   9 |  3000/ 4034 batches | ms/batch 34.78 | loss 520.4717\n",
      "| epoch   9 |  4000/ 4034 batches | ms/batch 38.32 | loss 572.1541\n",
      "| epoch   9 |  4034/ 4034 batches | ms/batch 26.96 | loss 387.7469\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 162.69s | loss = 394.6264 | NDCG@100 = 29.7539 | HR@100 = 49.0188 | Prec@100 = 6.81 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 162.69s | loss = 444.38 | NDCG@100 = 28.7473 | HR@100 = 47.3568 | Prec@100 = 6.503 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |  1000/ 4034 batches | ms/batch 34.68 | loss 503.7823\n",
      "| epoch  10 |  2000/ 4034 batches | ms/batch 35.09 | loss 524.7539\n",
      "| epoch  10 |  3000/ 4034 batches | ms/batch 32.57 | loss 518.8877\n",
      "| epoch  10 |  4000/ 4034 batches | ms/batch 36.12 | loss 570.1817\n",
      "| epoch  10 |  4034/ 4034 batches | ms/batch 24.64 | loss 386.7844\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 157.87s | loss = 394.396 | NDCG@100 = 29.8873 | HR@100 = 48.6561 | Prec@100 = 6.755 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 157.87s | loss = 444.7451 | NDCG@100 = 28.7184 | HR@100 = 47.396 | Prec@100 = 6.51 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |  1000/ 4034 batches | ms/batch 32.19 | loss 502.1377\n",
      "| epoch  11 |  2000/ 4034 batches | ms/batch 33.59 | loss 523.0211\n",
      "| epoch  11 |  3000/ 4034 batches | ms/batch 32.86 | loss 517.2376\n",
      "| epoch  11 |  4000/ 4034 batches | ms/batch 39.14 | loss 568.5662\n",
      "| epoch  11 |  4034/ 4034 batches | ms/batch 26.38 | loss 385.9964\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 157.28s | loss = 393.5801 | NDCG@100 = 29.5705 | HR@100 = 47.9517 | Prec@100 = 6.7 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 157.28s | loss = 445.0955 | NDCG@100 = 28.6344 | HR@100 = 47.3614 | Prec@100 = 6.49 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |  1000/ 4034 batches | ms/batch 35.09 | loss 500.6660\n",
      "| epoch  12 |  2000/ 4034 batches | ms/batch 36.19 | loss 521.6844\n",
      "| epoch  12 |  3000/ 4034 batches | ms/batch 35.62 | loss 516.0164\n",
      "| epoch  12 |  4000/ 4034 batches | ms/batch 39.07 | loss 567.0978\n",
      "| epoch  12 |  4034/ 4034 batches | ms/batch 27.62 | loss 384.7655\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 166.37s | loss = 393.0795 | NDCG@100 = 29.8974 | HR@100 = 48.5334 | Prec@100 = 6.765 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 166.37s | loss = 445.4299 | NDCG@100 = 28.5543 | HR@100 = 47.1666 | Prec@100 = 6.503 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |  1000/ 4034 batches | ms/batch 34.81 | loss 499.3279\n",
      "| epoch  13 |  2000/ 4034 batches | ms/batch 36.30 | loss 520.2152\n",
      "| epoch  13 |  3000/ 4034 batches | ms/batch 35.44 | loss 514.5963\n",
      "| epoch  13 |  4000/ 4034 batches | ms/batch 39.28 | loss 565.9389\n",
      "| epoch  13 |  4034/ 4034 batches | ms/batch 27.56 | loss 384.6043\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 165.87s | loss = 393.2573 | NDCG@100 = 29.6957 | HR@100 = 47.9049 | Prec@100 = 6.65 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 165.87s | loss = 446.1901 | NDCG@100 = 28.4058 | HR@100 = 46.9553 | Prec@100 = 6.449 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |  1000/ 4034 batches | ms/batch 35.23 | loss 498.1233\n",
      "| epoch  14 |  2000/ 4034 batches | ms/batch 36.37 | loss 518.9448\n",
      "| epoch  14 |  3000/ 4034 batches | ms/batch 35.36 | loss 513.7262\n",
      "| epoch  14 |  4000/ 4034 batches | ms/batch 39.58 | loss 564.7328\n",
      "| epoch  14 |  4034/ 4034 batches | ms/batch 27.29 | loss 383.6435\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 166.78s | loss = 392.5212 | NDCG@100 = 29.1471 | HR@100 = 47.7007 | Prec@100 = 6.675 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 166.78s | loss = 446.0438 | NDCG@100 = 28.741 | HR@100 = 47.4202 | Prec@100 = 6.539 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |  1000/ 4034 batches | ms/batch 35.19 | loss 497.1385\n",
      "| epoch  15 |  2000/ 4034 batches | ms/batch 36.35 | loss 517.7766\n",
      "| epoch  15 |  3000/ 4034 batches | ms/batch 35.45 | loss 512.8564\n",
      "| epoch  15 |  4000/ 4034 batches | ms/batch 39.19 | loss 563.7794\n",
      "| epoch  15 |  4034/ 4034 batches | ms/batch 27.18 | loss 382.3825\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 166.31s | loss = 391.9554 | NDCG@100 = 29.0458 | HR@100 = 47.4341 | Prec@100 = 6.6 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 166.31s | loss = 446.148 | NDCG@100 = 28.659 | HR@100 = 47.0185 | Prec@100 = 6.476 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |  1000/ 4034 batches | ms/batch 34.80 | loss 496.1086\n",
      "| epoch  16 |  2000/ 4034 batches | ms/batch 36.08 | loss 516.7605\n",
      "| epoch  16 |  3000/ 4034 batches | ms/batch 35.29 | loss 511.8208\n",
      "| epoch  16 |  4000/ 4034 batches | ms/batch 39.33 | loss 562.4533\n",
      "| epoch  16 |  4034/ 4034 batches | ms/batch 27.20 | loss 381.5274\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 165.53s | loss = 391.5771 | NDCG@100 = 29.5361 | HR@100 = 48.273 | Prec@100 = 6.76 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 165.53s | loss = 446.0862 | NDCG@100 = 28.8845 | HR@100 = 47.5077 | Prec@100 = 6.538 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |  1000/ 4034 batches | ms/batch 33.89 | loss 495.3041\n",
      "| epoch  17 |  2000/ 4034 batches | ms/batch 35.74 | loss 515.8573\n",
      "| epoch  17 |  3000/ 4034 batches | ms/batch 34.72 | loss 510.9173\n",
      "| epoch  17 |  4000/ 4034 batches | ms/batch 38.29 | loss 561.5503\n",
      "| epoch  17 |  4034/ 4034 batches | ms/batch 26.80 | loss 381.2714\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 162.93s | loss = 392.0045 | NDCG@100 = 28.9657 | HR@100 = 47.4609 | Prec@100 = 6.69 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 162.93s | loss = 447.387 | NDCG@100 = 28.6799 | HR@100 = 47.1536 | Prec@100 = 6.498 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |  1000/ 4034 batches | ms/batch 34.95 | loss 494.5478\n",
      "| epoch  18 |  2000/ 4034 batches | ms/batch 36.38 | loss 514.8631\n",
      "| epoch  18 |  3000/ 4034 batches | ms/batch 36.03 | loss 509.9290\n",
      "| epoch  18 |  4000/ 4034 batches | ms/batch 39.85 | loss 560.6376\n",
      "| epoch  18 |  4034/ 4034 batches | ms/batch 27.32 | loss 380.1269\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 167.34s | loss = 390.6304 | NDCG@100 = 29.2678 | HR@100 = 47.7739 | Prec@100 = 6.675 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 167.34s | loss = 446.5136 | NDCG@100 = 28.8381 | HR@100 = 47.4043 | Prec@100 = 6.53 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |  1000/ 4034 batches | ms/batch 35.05 | loss 493.6672\n",
      "| epoch  19 |  2000/ 4034 batches | ms/batch 36.26 | loss 513.9638\n",
      "| epoch  19 |  3000/ 4034 batches | ms/batch 33.69 | loss 509.0427\n",
      "| epoch  19 |  4000/ 4034 batches | ms/batch 38.25 | loss 559.9402\n",
      "| epoch  19 |  4034/ 4034 batches | ms/batch 26.39 | loss 379.7921\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 162.94s | loss = 389.9258 | NDCG@100 = 29.1997 | HR@100 = 48.0654 | Prec@100 = 6.74 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 162.94s | loss = 446.6137 | NDCG@100 = 28.6317 | HR@100 = 47.3328 | Prec@100 = 6.524 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  20 |  1000/ 4034 batches | ms/batch 34.34 | loss 492.7769\n",
      "| epoch  20 |  2000/ 4034 batches | ms/batch 35.60 | loss 513.2794\n",
      "| epoch  20 |  3000/ 4034 batches | ms/batch 34.85 | loss 508.3453\n",
      "| epoch  20 |  4000/ 4034 batches | ms/batch 38.21 | loss 559.0641\n",
      "| epoch  20 |  4034/ 4034 batches | ms/batch 26.59 | loss 378.9649\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 163.09s | loss = 390.2683 | NDCG@100 = 29.2329 | HR@100 = 48.5954 | Prec@100 = 6.725 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 163.09s | loss = 447.2678 | NDCG@100 = 28.7038 | HR@100 = 47.2365 | Prec@100 = 6.506 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |  1000/ 4034 batches | ms/batch 34.42 | loss 492.0454\n",
      "| epoch  21 |  2000/ 4034 batches | ms/batch 35.62 | loss 512.5124\n",
      "| epoch  21 |  3000/ 4034 batches | ms/batch 34.10 | loss 507.7614\n",
      "| epoch  21 |  4000/ 4034 batches | ms/batch 38.97 | loss 558.3332\n",
      "| epoch  21 |  4034/ 4034 batches | ms/batch 27.07 | loss 378.1768\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 162.91s | loss = 389.8355 | NDCG@100 = 29.1366 | HR@100 = 47.9267 | Prec@100 = 6.71 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 162.91s | loss = 447.4987 | NDCG@100 = 28.5337 | HR@100 = 47.0681 | Prec@100 = 6.464 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |  1000/ 4034 batches | ms/batch 34.29 | loss 491.5727\n",
      "| epoch  22 |  2000/ 4034 batches | ms/batch 36.03 | loss 511.7520\n",
      "| epoch  22 |  3000/ 4034 batches | ms/batch 34.93 | loss 507.1865\n",
      "| epoch  22 |  4000/ 4034 batches | ms/batch 38.58 | loss 557.8445\n",
      "| epoch  22 |  4034/ 4034 batches | ms/batch 29.31 | loss 378.0802\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 163.38s | loss = 389.6436 | NDCG@100 = 29.4673 | HR@100 = 48.635 | Prec@100 = 6.77 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 163.38s | loss = 447.8835 | NDCG@100 = 28.5581 | HR@100 = 47.232 | Prec@100 = 6.494 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |  1000/ 4034 batches | ms/batch 34.81 | loss 490.8549\n",
      "| epoch  23 |  2000/ 4034 batches | ms/batch 36.36 | loss 510.9126\n",
      "| epoch  23 |  3000/ 4034 batches | ms/batch 35.37 | loss 506.2168\n",
      "| epoch  23 |  4000/ 4034 batches | ms/batch 39.15 | loss 556.7558\n",
      "| epoch  23 |  4034/ 4034 batches | ms/batch 27.23 | loss 377.7329\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 165.17s | loss = 389.5583 | NDCG@100 = 29.6568 | HR@100 = 48.2494 | Prec@100 = 6.8 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 165.17s | loss = 448.7747 | NDCG@100 = 28.5058 | HR@100 = 46.9004 | Prec@100 = 6.465 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |  1000/ 4034 batches | ms/batch 35.06 | loss 490.7152\n",
      "| epoch  24 |  2000/ 4034 batches | ms/batch 36.06 | loss 510.5939\n",
      "| epoch  24 |  3000/ 4034 batches | ms/batch 34.71 | loss 506.0434\n",
      "| epoch  24 |  4000/ 4034 batches | ms/batch 38.22 | loss 556.0680\n",
      "| epoch  24 |  4034/ 4034 batches | ms/batch 26.82 | loss 376.7587\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 163.63s | loss = 388.6706 | NDCG@100 = 29.2421 | HR@100 = 48.247 | Prec@100 = 6.76 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 163.63s | loss = 448.0109 | NDCG@100 = 28.458 | HR@100 = 47.1365 | Prec@100 = 6.465 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |  1000/ 4034 batches | ms/batch 34.46 | loss 490.0235\n",
      "| epoch  25 |  2000/ 4034 batches | ms/batch 36.28 | loss 509.6938\n",
      "| epoch  25 |  3000/ 4034 batches | ms/batch 35.65 | loss 505.2842\n",
      "| epoch  25 |  4000/ 4034 batches | ms/batch 39.23 | loss 555.6364\n",
      "| epoch  25 |  4034/ 4034 batches | ms/batch 31.60 | loss 375.7988\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 165.51s | loss = 388.5365 | NDCG@100 = 29.7626 | HR@100 = 48.1346 | Prec@100 = 6.69 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 165.51s | loss = 448.1459 | NDCG@100 = 28.5987 | HR@100 = 47.0283 | Prec@100 = 6.477 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |  1000/ 4034 batches | ms/batch 35.08 | loss 489.2324\n",
      "| epoch  26 |  2000/ 4034 batches | ms/batch 36.29 | loss 509.2118\n",
      "| epoch  26 |  3000/ 4034 batches | ms/batch 35.53 | loss 504.5659\n",
      "| epoch  26 |  4000/ 4034 batches | ms/batch 38.79 | loss 555.2336\n",
      "| epoch  26 |  4034/ 4034 batches | ms/batch 26.79 | loss 376.6778\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 165.10s | loss = 388.3208 | NDCG@100 = 29.6472 | HR@100 = 48.3122 | Prec@100 = 6.82 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 165.10s | loss = 448.1614 | NDCG@100 = 28.4471 | HR@100 = 46.7799 | Prec@100 = 6.446 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |  1000/ 4034 batches | ms/batch 34.29 | loss 488.6712\n",
      "| epoch  27 |  2000/ 4034 batches | ms/batch 35.51 | loss 508.6221\n",
      "| epoch  27 |  3000/ 4034 batches | ms/batch 35.65 | loss 504.0432\n",
      "| epoch  27 |  4000/ 4034 batches | ms/batch 39.21 | loss 554.8486\n",
      "| epoch  27 |  4034/ 4034 batches | ms/batch 27.34 | loss 375.9287\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 164.84s | loss = 387.0933 | NDCG@100 = 29.3697 | HR@100 = 48.5139 | Prec@100 = 6.73 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 164.84s | loss = 448.0783 | NDCG@100 = 28.5246 | HR@100 = 47.2346 | Prec@100 = 6.482 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |  1000/ 4034 batches | ms/batch 34.86 | loss 488.2991\n",
      "| epoch  28 |  2000/ 4034 batches | ms/batch 36.43 | loss 508.1551\n",
      "| epoch  28 |  3000/ 4034 batches | ms/batch 35.05 | loss 503.5371\n",
      "| epoch  28 |  4000/ 4034 batches | ms/batch 38.63 | loss 553.9048\n",
      "| epoch  28 |  4034/ 4034 batches | ms/batch 27.62 | loss 375.3667\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 164.60s | loss = 387.114 | NDCG@100 = 29.2754 | HR@100 = 48.375 | Prec@100 = 6.77 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 164.60s | loss = 447.8195 | NDCG@100 = 28.4061 | HR@100 = 46.9179 | Prec@100 = 6.469 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |  1000/ 4034 batches | ms/batch 33.87 | loss 487.5728\n",
      "| epoch  29 |  2000/ 4034 batches | ms/batch 35.43 | loss 507.3872\n",
      "| epoch  29 |  3000/ 4034 batches | ms/batch 35.06 | loss 502.8941\n",
      "| epoch  29 |  4000/ 4034 batches | ms/batch 39.34 | loss 553.5586\n",
      "| epoch  29 |  4034/ 4034 batches | ms/batch 27.15 | loss 375.8296\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 164.34s | loss = 387.635 | NDCG@100 = 29.4517 | HR@100 = 48.6021 | Prec@100 = 6.73 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 164.34s | loss = 449.5761 | NDCG@100 = 28.0536 | HR@100 = 46.5255 | Prec@100 = 6.391 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  30 |  1000/ 4034 batches | ms/batch 35.01 | loss 487.1861\n",
      "| epoch  30 |  2000/ 4034 batches | ms/batch 36.34 | loss 506.9172\n",
      "| epoch  30 |  3000/ 4034 batches | ms/batch 35.44 | loss 502.5903\n",
      "| epoch  30 |  4000/ 4034 batches | ms/batch 39.28 | loss 553.5382\n",
      "| epoch  30 |  4034/ 4034 batches | ms/batch 27.24 | loss 374.1297\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 166.14s | loss = 387.0294 | NDCG@100 = 29.9705 | HR@100 = 48.5533 | Prec@100 = 6.695 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 166.14s | loss = 449.294 | NDCG@100 = 28.2062 | HR@100 = 46.6108 | Prec@100 = 6.42 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |  1000/ 4034 batches | ms/batch 34.69 | loss 486.5456\n",
      "| epoch  31 |  2000/ 4034 batches | ms/batch 35.62 | loss 506.4603\n",
      "| epoch  31 |  3000/ 4034 batches | ms/batch 34.89 | loss 502.1577\n",
      "| epoch  31 |  4000/ 4034 batches | ms/batch 38.35 | loss 552.6249\n",
      "| epoch  31 |  4034/ 4034 batches | ms/batch 27.18 | loss 373.8108\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 163.07s | loss = 386.3588 | NDCG@100 = 29.9558 | HR@100 = 49.0121 | Prec@100 = 6.75 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 163.07s | loss = 449.3057 | NDCG@100 = 28.3828 | HR@100 = 46.7979 | Prec@100 = 6.458 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |  1000/ 4034 batches | ms/batch 34.28 | loss 486.2285\n",
      "| epoch  32 |  2000/ 4034 batches | ms/batch 35.51 | loss 506.1583\n",
      "| epoch  32 |  3000/ 4034 batches | ms/batch 34.70 | loss 501.8941\n",
      "| epoch  32 |  4000/ 4034 batches | ms/batch 38.41 | loss 551.9082\n",
      "| epoch  32 |  4034/ 4034 batches | ms/batch 27.19 | loss 374.0658\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 162.37s | loss = 385.7479 | NDCG@100 = 29.3867 | HR@100 = 48.8328 | Prec@100 = 6.77 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 162.37s | loss = 449.0379 | NDCG@100 = 28.1293 | HR@100 = 46.4756 | Prec@100 = 6.46 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |  1000/ 4034 batches | ms/batch 34.43 | loss 485.6778\n",
      "| epoch  33 |  2000/ 4034 batches | ms/batch 36.24 | loss 505.5644\n",
      "| epoch  33 |  3000/ 4034 batches | ms/batch 35.55 | loss 501.1970\n",
      "| epoch  33 |  4000/ 4034 batches | ms/batch 39.19 | loss 551.5176\n",
      "| epoch  33 |  4034/ 4034 batches | ms/batch 27.60 | loss 373.6845\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 165.52s | loss = 386.2028 | NDCG@100 = 29.5512 | HR@100 = 48.5862 | Prec@100 = 6.675 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 165.52s | loss = 450.0018 | NDCG@100 = 28.1054 | HR@100 = 46.6954 | Prec@100 = 6.432 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |  1000/ 4034 batches | ms/batch 34.93 | loss 485.3404\n",
      "| epoch  34 |  2000/ 4034 batches | ms/batch 36.53 | loss 505.1029\n",
      "| epoch  34 |  3000/ 4034 batches | ms/batch 35.65 | loss 500.6174\n",
      "| epoch  34 |  4000/ 4034 batches | ms/batch 39.27 | loss 551.3088\n",
      "| epoch  34 |  4034/ 4034 batches | ms/batch 28.63 | loss 373.5836\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 166.83s | loss = 385.8392 | NDCG@100 = 29.567 | HR@100 = 48.6924 | Prec@100 = 6.69 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 166.83s | loss = 449.9731 | NDCG@100 = 28.2119 | HR@100 = 46.3712 | Prec@100 = 6.407 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |  1000/ 4034 batches | ms/batch 34.91 | loss 484.8224\n",
      "| epoch  35 |  2000/ 4034 batches | ms/batch 36.18 | loss 504.7223\n",
      "| epoch  35 |  3000/ 4034 batches | ms/batch 35.96 | loss 499.9038\n",
      "| epoch  35 |  4000/ 4034 batches | ms/batch 39.19 | loss 550.6652\n",
      "| epoch  35 |  4034/ 4034 batches | ms/batch 27.39 | loss 373.8849\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 166.44s | loss = 386.0451 | NDCG@100 = 29.725 | HR@100 = 48.7286 | Prec@100 = 6.725 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 166.44s | loss = 450.6355 | NDCG@100 = 28.0817 | HR@100 = 46.7547 | Prec@100 = 6.431 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |  1000/ 4034 batches | ms/batch 34.94 | loss 484.7747\n",
      "| epoch  36 |  2000/ 4034 batches | ms/batch 36.28 | loss 504.1254\n",
      "| epoch  36 |  3000/ 4034 batches | ms/batch 35.71 | loss 499.5516\n",
      "| epoch  36 |  4000/ 4034 batches | ms/batch 39.23 | loss 550.0023\n",
      "| epoch  36 |  4034/ 4034 batches | ms/batch 27.64 | loss 372.9746\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 165.84s | loss = 385.4712 | NDCG@100 = 29.4798 | HR@100 = 48.8137 | Prec@100 = 6.715 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 165.84s | loss = 450.682 | NDCG@100 = 27.978 | HR@100 = 46.5301 | Prec@100 = 6.376 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |  1000/ 4034 batches | ms/batch 34.67 | loss 483.9953\n",
      "| epoch  37 |  2000/ 4034 batches | ms/batch 36.27 | loss 503.9581\n",
      "| epoch  37 |  3000/ 4034 batches | ms/batch 35.27 | loss 499.1601\n",
      "| epoch  37 |  4000/ 4034 batches | ms/batch 38.47 | loss 549.7407\n",
      "| epoch  37 |  4034/ 4034 batches | ms/batch 25.75 | loss 371.5818\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 164.10s | loss = 385.0273 | NDCG@100 = 29.4264 | HR@100 = 48.8712 | Prec@100 = 6.725 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 164.10s | loss = 450.503 | NDCG@100 = 27.9647 | HR@100 = 46.488 | Prec@100 = 6.439 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |  1000/ 4034 batches | ms/batch 33.90 | loss 483.9415\n",
      "| epoch  38 |  2000/ 4034 batches | ms/batch 35.40 | loss 503.4986\n",
      "| epoch  38 |  3000/ 4034 batches | ms/batch 35.52 | loss 499.0357\n",
      "| epoch  38 |  4000/ 4034 batches | ms/batch 39.02 | loss 549.5550\n",
      "| epoch  38 |  4034/ 4034 batches | ms/batch 27.20 | loss 370.7765\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 163.99s | loss = 385.2512 | NDCG@100 = 29.7362 | HR@100 = 49.9707 | Prec@100 = 6.785 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 163.99s | loss = 451.0264 | NDCG@100 = 28.2498 | HR@100 = 46.9985 | Prec@100 = 6.477 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |  1000/ 4034 batches | ms/batch 35.13 | loss 483.1986\n",
      "| epoch  39 |  2000/ 4034 batches | ms/batch 36.32 | loss 503.1211\n",
      "| epoch  39 |  3000/ 4034 batches | ms/batch 35.55 | loss 498.7687\n",
      "| epoch  39 |  4000/ 4034 batches | ms/batch 38.79 | loss 548.9338\n",
      "| epoch  39 |  4034/ 4034 batches | ms/batch 27.44 | loss 371.2407\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 166.00s | loss = 385.3625 | NDCG@100 = 29.3744 | HR@100 = 49.7208 | Prec@100 = 6.815 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 166.00s | loss = 451.2034 | NDCG@100 = 27.9103 | HR@100 = 46.5903 | Prec@100 = 6.441 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  40 |  1000/ 4034 batches | ms/batch 34.93 | loss 482.8386\n",
      "| epoch  40 |  2000/ 4034 batches | ms/batch 36.70 | loss 502.9041\n",
      "| epoch  40 |  3000/ 4034 batches | ms/batch 35.42 | loss 498.2710\n",
      "| epoch  40 |  4000/ 4034 batches | ms/batch 38.94 | loss 548.5096\n",
      "| epoch  40 |  4034/ 4034 batches | ms/batch 26.98 | loss 370.5441\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 165.38s | loss = 384.8039 | NDCG@100 = 29.7526 | HR@100 = 49.5306 | Prec@100 = 6.79 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 165.38s | loss = 451.3204 | NDCG@100 = 28.0736 | HR@100 = 46.8821 | Prec@100 = 6.48 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |  1000/ 4034 batches | ms/batch 33.94 | loss 482.7340\n",
      "| epoch  41 |  2000/ 4034 batches | ms/batch 35.47 | loss 502.5527\n",
      "| epoch  41 |  3000/ 4034 batches | ms/batch 34.57 | loss 497.6908\n",
      "| epoch  41 |  4000/ 4034 batches | ms/batch 38.31 | loss 548.3557\n",
      "| epoch  41 |  4034/ 4034 batches | ms/batch 27.21 | loss 370.9448\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 161.80s | loss = 384.517 | NDCG@100 = 29.4375 | HR@100 = 49.6649 | Prec@100 = 6.835 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 161.80s | loss = 451.1364 | NDCG@100 = 28.0852 | HR@100 = 46.9075 | Prec@100 = 6.447 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |  1000/ 4034 batches | ms/batch 34.05 | loss 482.5176\n",
      "| epoch  42 |  2000/ 4034 batches | ms/batch 36.05 | loss 502.2231\n",
      "| epoch  42 |  3000/ 4034 batches | ms/batch 35.10 | loss 497.3414\n",
      "| epoch  42 |  4000/ 4034 batches | ms/batch 38.85 | loss 547.9295\n",
      "| epoch  42 |  4034/ 4034 batches | ms/batch 27.38 | loss 369.9945\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 164.21s | loss = 383.7877 | NDCG@100 = 29.2632 | HR@100 = 49.5902 | Prec@100 = 6.69 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 164.21s | loss = 450.7055 | NDCG@100 = 27.8817 | HR@100 = 46.7615 | Prec@100 = 6.409 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |  1000/ 4034 batches | ms/batch 35.02 | loss 481.8236\n",
      "| epoch  43 |  2000/ 4034 batches | ms/batch 36.17 | loss 501.6219\n",
      "| epoch  43 |  3000/ 4034 batches | ms/batch 35.57 | loss 497.2366\n",
      "| epoch  43 |  4000/ 4034 batches | ms/batch 39.06 | loss 546.9378\n",
      "| epoch  43 |  4034/ 4034 batches | ms/batch 26.62 | loss 369.7910\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 165.26s | loss = 384.1042 | NDCG@100 = 29.6129 | HR@100 = 49.677 | Prec@100 = 6.76 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 165.26s | loss = 451.6819 | NDCG@100 = 27.8754 | HR@100 = 47.0953 | Prec@100 = 6.47 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |  1000/ 4034 batches | ms/batch 34.58 | loss 481.7037\n",
      "| epoch  44 |  2000/ 4034 batches | ms/batch 35.53 | loss 501.5054\n",
      "| epoch  44 |  3000/ 4034 batches | ms/batch 35.24 | loss 496.8645\n",
      "| epoch  44 |  4000/ 4034 batches | ms/batch 39.22 | loss 546.5335\n",
      "| epoch  44 |  4034/ 4034 batches | ms/batch 27.25 | loss 369.6784\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 164.80s | loss = 385.1139 | NDCG@100 = 29.4944 | HR@100 = 49.997 | Prec@100 = 6.75 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 164.80s | loss = 452.3251 | NDCG@100 = 27.6623 | HR@100 = 46.5131 | Prec@100 = 6.386 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |  1000/ 4034 batches | ms/batch 35.15 | loss 481.4980\n",
      "| epoch  45 |  2000/ 4034 batches | ms/batch 36.73 | loss 500.7847\n",
      "| epoch  45 |  3000/ 4034 batches | ms/batch 35.52 | loss 496.1515\n",
      "| epoch  45 |  4000/ 4034 batches | ms/batch 38.96 | loss 546.7880\n",
      "| epoch  45 |  4034/ 4034 batches | ms/batch 26.54 | loss 369.5553\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 165.72s | loss = 384.4627 | NDCG@100 = 29.256 | HR@100 = 49.608 | Prec@100 = 6.695 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 165.72s | loss = 452.2015 | NDCG@100 = 28.0832 | HR@100 = 46.8535 | Prec@100 = 6.419 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |  1000/ 4034 batches | ms/batch 34.59 | loss 481.0202\n",
      "| epoch  46 |  2000/ 4034 batches | ms/batch 36.55 | loss 500.4431\n",
      "| epoch  46 |  3000/ 4034 batches | ms/batch 35.76 | loss 496.0650\n",
      "| epoch  46 |  4000/ 4034 batches | ms/batch 39.11 | loss 546.5019\n",
      "| epoch  46 |  4034/ 4034 batches | ms/batch 27.62 | loss 369.2071\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 166.15s | loss = 385.1977 | NDCG@100 = 29.2294 | HR@100 = 49.6584 | Prec@100 = 6.76 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 166.15s | loss = 452.7204 | NDCG@100 = 27.7609 | HR@100 = 46.5451 | Prec@100 = 6.411 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |  1000/ 4034 batches | ms/batch 35.08 | loss 480.4753\n",
      "| epoch  47 |  2000/ 4034 batches | ms/batch 36.46 | loss 500.2278\n"
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "    total_anneal_steps = 200000\n",
    "    anneal = 0.0\n",
    "    update_count = 0.0\n",
    "    anneal_cap = 0.2\n",
    "\n",
    "    for x, y_s, y in reader.iter():\n",
    "        # print(x[0])\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_output, z_mean, z_log_sigma = model(x)\n",
    "        \n",
    "        loss = criterion(decoder_output, z_mean, z_log_sigma, y_s, y, anneal)\n",
    "        loss.backward()\n",
    "        \n",
    "        # nn.utils.clip_grad_norm(model.parameters(), hyper_params['exploding_clip'])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "        if total_anneal_steps > 0:\n",
    "            anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "        else:\n",
    "            anneal = anneal_cap\n",
    "        update_count += 1.0\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_items = load_data(hyper_params)\n",
    "# print(train_reader.data[:10])\n",
    "# print(test_reader.data[:10])\n",
    "# print(test_reader.data_te[:10])\n",
    "hyper_params['total_items'] = total_items\n",
    "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "# file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "        metrics = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "        string = \"\"\n",
    "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "# Checking metrics on best saved model\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "# Plot Traning graph\n",
    "f = open(model.hyper_params['log_file'])\n",
    "lines = f.readlines()\n",
    "lines.reverse()\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[:10] == 'Simulation' and len(train) > 0: break\n",
    "    if line[2:5] == 'end' and line[-6:-2] == 'TEST': test.append(line.strip().split(\"|\"))\n",
    "    elif line[2:5] == 'end' and line[-7:-2] == 'TRAIN': train.append(line.strip().split(\"|\"))\n",
    "\n",
    "train.reverse()\n",
    "test.reverse()\n",
    "\n",
    "train_cp, train_ndcg = [], []\n",
    "test_cp, test_ndcg = [], []\n",
    "\n",
    "for i in train:\n",
    "    train_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    train_ndcg.append(float(i[-2].split('=')[1].split(' ')[1]))\n",
    "    \n",
    "for i in test:\n",
    "    test_cp.append(float(i[3].split('=')[1].strip(' ')))\n",
    "    test_ndcg.append(float(i[-2].split('=')[1].split(' ')[1]))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_ndcg, label='Train')\n",
    "plt.plot(test_ndcg, label='Test')\n",
    "plt.ylabel(\"NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "leg = plt.legend(loc='best', ncol=2)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
