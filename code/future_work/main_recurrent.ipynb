{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'data_base': 'saved_data/',\n",
    "    'project_name': 'ranking_vae_recurrent',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'data_split': [0.8, 0.2], # Train : Test\n",
    "    'max_user_hist': 100,\n",
    "    'min_user_hist': 5,\n",
    "\n",
    "    'learning_rate': 0.05, # if optimizer is adadelta, learning rate is not required\n",
    "    'optimizer': 'adam',\n",
    "    'loss_type': 'hinge',\n",
    "    'm_loss': float(1),\n",
    "    'weight_decay': float(1e-4),\n",
    "\n",
    "    'epochs': 50,\n",
    "    'batch_size': 1,\n",
    "\n",
    "    'user_embed_size': 50,\n",
    "    'item_embed_size': 50,\n",
    "    'rnn_size': 16,\n",
    "    \n",
    "    'hidden_size': 64,\n",
    "    'latent_size': 32,\n",
    "\n",
    "    'number_users_to_keep': 100000000000,\n",
    "    'batch_log_interval': 500,\n",
    "    'train_cp_users': 200,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] != 'adadelta':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_user_embed_size_' + str(hyper_params['user_embed_size'])\n",
    "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "\n",
    "hyper_params['log_file'] = 'saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = 'saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    \n",
    "    train = load_obj_json(hyper_params['data_base'] + 'train_ranking_vae')\n",
    "    test = load_obj_json(hyper_params['data_base'] + 'test_ranking_vae')\n",
    "    user_hist = load_obj_json(hyper_params['data_base'] + 'user_hist_ranking_vae')\n",
    "    item_hist = load_obj_json(hyper_params['data_base'] + 'item_hist_ranking_vae')\n",
    "\n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "\n",
    "    train_reader = DataReader(hyper_params, train, len(user_hist), item_hist, True)\n",
    "    test_reader = DataReader(hyper_params, test, len(user_hist), item_hist, False, train)\n",
    "\n",
    "    return train_reader, test_reader, len(user_hist), len(item_hist)\n",
    "\n",
    "def scatter(x):\n",
    "    max_len = max(list(map(len, x)))\n",
    "    \n",
    "    this = 0.0\n",
    "    if type(x[0][0]) == int: this = 0\n",
    "\n",
    "    ret = [[this] * max_len] * len(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            ret[i][j] = x[i][j]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, hyper_params, data, num_users, item_hist, is_training, context=None):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.item_hist = item_hist\n",
    "        self.num_users = num_users\n",
    "        self.num_items = len(item_hist)\n",
    "        self.data = data\n",
    "        self.is_training = is_training\n",
    "        self.all_users = []\n",
    "        self.context = context\n",
    "\n",
    "        self.number_users()\n",
    "        self.number()\n",
    "\n",
    "    def number(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "        y_batch = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            y_batch.append(0)\n",
    "\n",
    "            if len(y_batch) == self.batch_size:\n",
    "                y_batch = []\n",
    "                count += 1\n",
    "\n",
    "        self.num_b = count\n",
    "\n",
    "    def number_users(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            count += 1\n",
    "            self.all_users.append(user)\n",
    "\n",
    "        self.num_u = count\n",
    "\n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch_user = []\n",
    "        x_batch_item = []\n",
    "        y_batch = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "            \n",
    "            x_batch_user2 = []\n",
    "            x_batch_item2 = []\n",
    "            y_batch2 = []\n",
    "            \n",
    "            for i in range(len(self.data[user])):\n",
    "                x_batch_user2.append(int(user))\n",
    "                x_batch_item2.append(self.data[user][i][0])\n",
    "                y_batch2.append(float(self.data[user][i][1]))\n",
    "                \n",
    "            x_batch_user.append(x_batch_user2)\n",
    "            x_batch_item.append(x_batch_item2)\n",
    "            y_batch.append(y_batch2)\n",
    "\n",
    "            if len(y_batch) == self.batch_size:\n",
    "\n",
    "                yield [\n",
    "                    Variable(LongTensor(scatter(x_batch_user))), \n",
    "                    Variable(LongTensor(scatter(x_batch_item)))\n",
    "                ], y_batch\n",
    "\n",
    "                x_batch_user = []\n",
    "                x_batch_item = []\n",
    "                y_batch = []\n",
    "\n",
    "    def iter_eval(self):\n",
    "\n",
    "        num_active_users = len(self.all_users)\n",
    "        num_active_users = min(num_active_users, self.hyper_params['number_users_to_keep'])\n",
    "\n",
    "        for user_now in tqdm(range(num_active_users)):\n",
    "            user = self.all_users[user_now]\n",
    "\n",
    "            all_movies = []\n",
    "            x_batch_user2 = []\n",
    "            x_batch_item2 = []\n",
    "            y_batch2 = []\n",
    "            \n",
    "            if self.context is None: ad = []\n",
    "            else: ad = self.context[user]\n",
    "            \n",
    "            for i in range(len(self.data[user])):\n",
    "                all_movies.append(self.data[user][i][0])\n",
    "            \n",
    "            for i in range(len(ad + self.data[user])):\n",
    "                x_batch_user2.append(int(user))\n",
    "                x_batch_item2.append((ad + self.data[user])[i][0])\n",
    "                y_batch2.append(float((ad + self.data[user])[i][1]))\n",
    "                \n",
    "            yield [\n",
    "                Variable(LongTensor([ x_batch_user2 ])), \n",
    "                Variable(LongTensor([ x_batch_item2 ]))\n",
    "            ], [ y_batch2 ], all_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_int(a):\n",
    "    if float(a.data) < 0.0: return -1\n",
    "    if float(a.data) > 0.0: return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_ndcg(model, criterion, reader, hyper_params):\n",
    "    model.eval()\n",
    "\n",
    "    ret = 0.0\n",
    "    \n",
    "    NDCG = {}\n",
    "    total_NDCG = {}\n",
    "\n",
    "    Ks = [1, 5, 10, 15, 20]\n",
    "\n",
    "    for k in Ks:\n",
    "        NDCG[str(k)] = 0.0\n",
    "        total_NDCG[str(k)] = 0.0\n",
    "\n",
    "    total = 0\n",
    "    user_done = 0\n",
    "\n",
    "    for x, y, all_movies in reader.iter_eval():\n",
    "        user_done += 1\n",
    "\n",
    "        _, o = model(x)\n",
    "\n",
    "        y_pair_map = {}\n",
    "        out_pair_map = {}\n",
    "        y_r = {}\n",
    "        \n",
    "        for user in range(len(y)):\n",
    "            for i in range(len(y[user])):\n",
    "                for j in range(i+1, len(y[user])):\n",
    "                    m1 = int(x[1][user][i].data)\n",
    "                    m2 = int(x[1][user][j].data)\n",
    "                    \n",
    "                    y_r[str(m1)] = round(float(y[user][i]), 2)\n",
    "                    y_r[str(m2)] = round(float(y[user][j]), 2)\n",
    "                    \n",
    "                    if y[user][i] == y[user][j]: continue\n",
    "                    \n",
    "                    result_y = 1\n",
    "                    if y[user][i] < y[user][j]: result_y = -1\n",
    "                    \n",
    "                    result_out = 1\n",
    "                    if float(o[user][i].data) < float(o[user][j].data): result_out = -1\n",
    "\n",
    "                    if m1 not in y_pair_map: y_pair_map[m1] = {}\n",
    "                    if m2 not in y_pair_map: y_pair_map[m2] = {}\n",
    "                    if m1 not in out_pair_map: out_pair_map[m1] = {}\n",
    "                    if m2 not in out_pair_map: out_pair_map[m2] = {}\n",
    "\n",
    "                    y_pair_map[m1][m2] = result_y\n",
    "                    y_pair_map[m2][m1] = -1 * result_y\n",
    "\n",
    "                    out_pair_map[m1][m2] = result_out\n",
    "                    out_pair_map[m2][m1] = -1 * result_out\n",
    "\n",
    "        def compare_out(item1, item2):\n",
    "            if item1 not in out_pair_map: return 0\n",
    "            if item2 not in out_pair_map[item1]: return 0\n",
    "            return out_pair_map[item1][item2]\n",
    "\n",
    "        all_movies = sorted(all_movies, key=functools.cmp_to_key(compare_out))\n",
    "        all_movies.reverse()\n",
    "        \n",
    "        final = []\n",
    "        final_sorted = []\n",
    "        for i in all_movies: \n",
    "            final.append([i, y_r[str(i)]])\n",
    "            final_sorted.append([i, y_r[str(i)]])\n",
    "        final_sorted = sorted(final_sorted, key=lambda xx: xx[1])\n",
    "        final_sorted.reverse()\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        for k in Ks:\n",
    "            if k <= len(final):\n",
    "                out = final[:k]\n",
    "                out_sorted = final_sorted[:k]\n",
    "\n",
    "                now = 0.0\n",
    "                now_best = 0.0\n",
    "                for i in range(k):\n",
    "                    now += float(out[i][1]) / float(np.log2(i+2))\n",
    "                    now_best += float(out_sorted[i][1]) / float(np.log2(i+2))\n",
    "\n",
    "                NDCG[str(k)] += float(now) / float(now_best)\n",
    "                total_NDCG[str(k)] += 1.0\n",
    "\n",
    "    for k in NDCG:\n",
    "        if total_NDCG[k] > 0: NDCG[k] = float(NDCG[k]) / float(total_NDCG[k])\n",
    "        NDCG[k] *= 100.0\n",
    "        NDCG[k] = round(NDCG[k], 4)\n",
    "\n",
    "    return NDCG\n",
    "\n",
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['CP'] = 0.0\n",
    "    metrics['ZEROS'] = 0.0\n",
    "    metrics['loss'] = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    \n",
    "    temp_zer = Variable(FloatTensor([0.0]))\n",
    "\n",
    "    for x, y in reader.iter():\n",
    "        batch += 1\n",
    "        if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
    "\n",
    "        o1, o = model(x)\n",
    "        \n",
    "        loss_now = 0.0\n",
    "        total2 = 0.0\n",
    "        temp_correct, temp_not_correct, temp_zeros = 0.0, 0.0, 0.0\n",
    "        \n",
    "        for user in range(len(y)):\n",
    "            for i in range(len(y[user])):\n",
    "                for j in range(i+1, len(y[user])):\n",
    "                    if y[user][i] == y[user][j]: continue\n",
    "                    \n",
    "                    y_temp = 1.0\n",
    "                    if y[user][i] < y[user][j]: y_temp = -1.0\n",
    "                    loss_now += float(torch.max(temp_zer, hyper_params['m_loss'] - (y_temp * (o[user][i] - o[user][j]))).data)\n",
    "                    total2 += 1.0\n",
    "                    \n",
    "                    result = float((y_temp * (o[user][i] - o[user][j])).data)\n",
    "                    \n",
    "                    if result > 0.0: temp_correct += 1\n",
    "                    elif result < 0.0: temp_not_correct += 1\n",
    "                    else: temp_zeros += 1\n",
    "        \n",
    "        assert temp_correct + temp_not_correct + temp_zeros == total2\n",
    "        \n",
    "        if total2 > 0.0: metrics['loss'] += loss_now / total2\n",
    "        \n",
    "        correct += temp_correct\n",
    "        not_correct += temp_not_correct\n",
    "        zeros += temp_zeros\n",
    "        total += total2\n",
    "\n",
    "    assert correct + not_correct + zeros == total\n",
    "\n",
    "    metrics['CP'] = float(correct) / float(total)\n",
    "    metrics['CP'] *= 100.0\n",
    "    metrics['CP'] = round(metrics['CP'], 4)\n",
    "\n",
    "    metrics['ZEROS'] = float(zeros) / float(total)\n",
    "    metrics['ZEROS'] *= 100.0\n",
    "    metrics['ZEROS'] = round(metrics['ZEROS'], 4)\n",
    "\n",
    "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
    "    metrics['loss'] = round(metrics['loss'], 4)\n",
    "\n",
    "    if is_train_set == False:\n",
    "        ndcg = evaluate_ndcg(model, criterion, reader, hyper_params)\n",
    "        for k in ndcg: metrics['NDCG@' + str(k)] = ndcg[k]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['user_embed_size'] + hyper_params['item_embed_size'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder = Decoder(hyper_params)\n",
    "        \n",
    "        self.gru = nn.GRU(hyper_params['latent_size'], hyper_params['rnn_size'], batch_first=True, num_layers=1)\n",
    "        \n",
    "        self._enc_mu = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        self._enc_log_sigma = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self._enc_mu.weight)\n",
    "        nn.init.xavier_normal(self._enc_log_sigma.weight)\n",
    "        \n",
    "        self.user_embed = nn.Embedding(hyper_params['total_users'], hyper_params['user_embed_size'])\n",
    "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
    "        nn.init.normal(self.user_embed.weight.data, mean=0, std=0.01)\n",
    "        nn.init.normal(self.item_embed.weight.data, mean=0, std=0.01)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation_last = nn.Tanh()\n",
    "        if self.hyper_params['loss_type'] == 'bce': self.activation_last = nn.Sigmoid()\n",
    "        \n",
    "        prev = hyper_params['rnn_size']\n",
    "        self.layer_hinge1 = nn.Linear(prev, 1)\n",
    "#         self.layer_hinge2 = nn.Linear(64, 1)\n",
    "        nn.init.xavier_normal(self.layer_hinge1.weight)\n",
    "#         nn.init.xavier_normal(self.layer_hinge2.weight)\n",
    "        # xavier_uniform\n",
    "        \n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "        if is_cuda_available: std_z = std_z.cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x[0].shape\n",
    "        \n",
    "        user = self.user_embed(x[0]).view(x_shape[0] * x_shape[1], -1)\n",
    "        item = self.item_embed(x[1]).view(x_shape[0] * x_shape[1], -1)\n",
    "        \n",
    "        h_enc = self.encoder(torch.cat([user, item], dim=-1))\n",
    "        z = self.sample_latent(h_enc)\n",
    "        dec = self.decoder(z)\n",
    "        \n",
    "        z = z.view(x_shape[0], x_shape[1], -1)\n",
    "        \n",
    "        output, _ = self.gru(z)\n",
    "        \n",
    "        output = output.contiguous().view(x_shape[0] * x_shape[1], -1)\n",
    "        \n",
    "        output = self.layer_hinge1(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.layer_hinge2(output)\n",
    "        output = self.activation_last(output)\n",
    "                              \n",
    "        return [\n",
    "            dec, self.z_mean, self.z_sigma\n",
    "        ], output.squeeze(-1).view(x_shape[0], x_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "\n",
    "        self.loss_type = hyper_params['loss_type']\n",
    "        self.m_loss = hyper_params['m_loss']\n",
    "        batch_size = hyper_params['batch_size']\n",
    "\n",
    "        self.zeros_while_max = torch.zeros(int(batch_size)).float()\n",
    "        if is_cuda_available: self.zeros_while_max = self.zeros_while_max.cuda()\n",
    "        self.zeros_while_max = Variable(self.zeros_while_max)\n",
    "        self.zer = Variable(FloatTensor([0.0]))\n",
    "        self.exp = Variable(FloatTensor([np.e]))\n",
    "        self.hundred = Variable(FloatTensor([100.0]))\n",
    "        self.cce = nn.CrossEntropyLoss(size_average=True)\n",
    "        self.bce = nn.BCELoss(size_average=True)\n",
    "\n",
    "    def forward(self, o1, o, y, t):\n",
    "        \n",
    "        m, zm, zs = o1\n",
    "        \n",
    "        mean_sq = zm * zm\n",
    "        stddev_sq = zs * zs\n",
    "        kld = torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n",
    "        \n",
    "#         print(m.shape, t.shape)\n",
    "        likelihood = self.cce(m, t) # Try using a weight parameters, alpha when summing\n",
    "        \n",
    "        total = 0\n",
    "        for user in range(len(y)):\n",
    "            for i in range(len(y[user])):\n",
    "                for j in range(i+1, len(y[user])):\n",
    "                    if y[user][i] == y[user][j]: continue\n",
    "                    total += 1\n",
    "        \n",
    "        if total == 0: total = 1\n",
    "        pairwise = Variable(FloatTensor([0.0] * total))\n",
    "        total = 0\n",
    "        for user in range(len(y)):\n",
    "            for i in range(len(y[user])):\n",
    "                for j in range(i+1, len(y[user])):\n",
    "                    if y[user][i] == y[user][j]: continue\n",
    "                    \n",
    "                    y_temp = 1.0\n",
    "                    if y[user][i] < y[user][j]: y_temp = -1.0\n",
    "#                     if pairwise == None: pairwise = torch.max(self.zer, self.m_loss - (y_temp * (o[user][i] - o[user][j])))\n",
    "                    pairwise[total] = torch.max(self.zer, self.m_loss - (y_temp * (o[user][i] - o[user][j])))\n",
    "                    total += 1\n",
    "        \n",
    "        final = (0.1 * kld) + (0.15 * likelihood) + (0.75 * torch.mean(pairwise))\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n",
      "\n",
      "\n",
      "Simulation run on: 2018-07-12 10:06:05.077369\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches: 3131\n",
      "Number of test batches: 3131\n",
      "Total Users: 3131\n",
      "Total Items: 3274\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (linear2): Linear(in_features=64, out_features=3274, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_enc_log_sigma): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (user_embed): Embedding(3131, 50)\n",
      "  (item_embed): Embedding(3274, 50)\n",
      "  (activation): ReLU()\n",
      "  (activation_last): Tanh()\n",
      "  (layer_hinge1): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n",
      "| epoch   1 |   500/ 3131 batches | ms/batch 108.87 | loss 1.8780\n",
      "| epoch   1 |  1000/ 3131 batches | ms/batch 106.84 | loss 1.7993\n",
      "| epoch   1 |  1500/ 3131 batches | ms/batch 110.68 | loss 1.7891\n",
      "| epoch   1 |  2000/ 3131 batches | ms/batch 125.69 | loss 1.7701\n",
      "| epoch   1 |  2500/ 3131 batches | ms/batch 114.48 | loss 1.7524\n",
      "| epoch   1 |  3000/ 3131 batches | ms/batch 121.49 | loss 1.7408\n",
      "| epoch   1 |  3131/ 3131 batches | ms/batch 119.31 | loss 1.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3131/3131 [03:00<00:00, 17.32it/s]\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 562.07s | CP = 66.0663 | ZEROS = 0.0 | loss = 0.7504 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 562.07s | CP = 59.9712 | ZEROS = 0.0 | loss = 0.8533 | NDCG@1 = 83.9274 | NDCG@5 = 88.5888 | NDCG@10 = 90.1618 | NDCG@15 = 92.2361 | NDCG@20 = 95.2367 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   500/ 3131 batches | ms/batch 119.22 | loss 1.6851\n",
      "| epoch   2 |  1000/ 3131 batches | ms/batch 115.35 | loss 1.6656\n",
      "| epoch   2 |  1500/ 3131 batches | ms/batch 111.51 | loss 1.6530\n",
      "| epoch   2 |  2000/ 3131 batches | ms/batch 121.29 | loss 1.6330\n",
      "| epoch   2 |  2500/ 3131 batches | ms/batch 111.81 | loss 1.6090\n",
      "| epoch   2 |  3000/ 3131 batches | ms/batch 118.41 | loss 1.5928\n",
      "| epoch   2 |  3131/ 3131 batches | ms/batch 117.11 | loss 1.5998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3131/3131 [02:57<00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 561.84s | CP = 67.7674 | ZEROS = 0.0 | loss = 0.7149 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 561.84s | CP = 60.6866 | ZEROS = 0.0 | loss = 0.8595 | NDCG@1 = 83.526 | NDCG@5 = 88.6604 | NDCG@10 = 90.2273 | NDCG@15 = 92.0943 | NDCG@20 = 94.7577 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   500/ 3131 batches | ms/batch 115.62 | loss 1.5381\n",
      "Exiting from training early\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6660715cfbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e04fa2509dad>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, reader, hyper_params, is_train_set)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train_set\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_cp_users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mloss_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/noveen/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-91edda90436b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mh_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/noveen/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0;34m\"forward hooks should never return any values, but '{}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                     \"didn't return None\".format(hook))\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "\n",
    "    for x, y in reader.iter():\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        temp_o1, temp_o2 = model(x)\n",
    "        \n",
    "        loss = criterion(temp_o1, temp_o2, y, x[1].view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_users, total_items = load_data(hyper_params)\n",
    "hyper_params['total_users'] = total_users\n",
    "hyper_params['total_items'] = total_items\n",
    "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "        metrics = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "        string = \"\"\n",
    "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
