{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for ranking items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Formalization\n",
    "\n",
    "For each user $u \\in U$, we have a set, $P_u$ = { $(m_1, m_2)$ | $rating_u^{m_1}$ > $rating_u^{m_2}$) } \n",
    "\n",
    "$P$ =  $\\bigcup\\limits_{\\forall u \\; \\in \\; U} P_u$\n",
    "\n",
    "$\\forall (u, m_1, m_2) \\in P, $ we send two inputs, $x_1 = u \\Vert m_1$ and $x_2 = u \\Vert m_2$ to a VAE (with the same parameters).\n",
    "\n",
    "We expect the VAE's encoder to produce $z_1$ (sampled from the distribution: $(\\mu_1 , \\Sigma_1$)) from $x_1$ ; and similarly $z_2$ from $x_2$ using the parameters $\\theta$.\n",
    "\n",
    "The decoder network is expected to learn a mapping function $f_{\\phi}$ from $z_1$ to $m_1$.\n",
    "\n",
    "We currently have 2 ideas for the decoder network:\n",
    "1. Using two sets of network parameters, $\\phi$ and $\\psi$ for $z_1$ and $z_2$ respectively.\n",
    "2. Using $\\phi$ for both $z_1$ and $z_2$.\n",
    "\n",
    "For ranking the pairs of movies, we have another network:\n",
    "1. The input of the network is $z_1 \\Vert z_2$, \n",
    "2. Is expected to learn a mapping, $f_{\\delta}$ to a bernoulli distribution over True/False, modelling $rating_u^{m_1} > rating_u^{m_2}$.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$Loss \\; = \\; KL( \\, \\phi(z_1 \\vert x_1) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; + \\; KL( \\, \\psi(z_2 \\vert x_2) \\Vert {\\rm I\\!N(0, I)} \\, ) \\; - \\; \\sum_{i} m_{1i} \\, log( \\, f_{\\phi}(z_1)_i ) \\; - \\; \\sum_{i} m_{2i} \\, log( \\, f_{\\psi}(z_2)_i ) \\; - \\; f_{\\delta}(z_1 \\Vert z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LongTensor = torch.LongTensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_obj_json(obj, name):\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file_write(log_file, s):\n",
    "    print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'data_base': 'saved_data/',\n",
    "    'project_name': 'ranking_vae_single_score',\n",
    "    'model_file_name': '',\n",
    "    'log_file': '',\n",
    "    'data_split': [0.8, 0.2], # Train : Test\n",
    "    'max_user_hist': 100,\n",
    "    'min_user_hist': 5,\n",
    "\n",
    "    'learning_rate': 0.05, # if optimizer is adadelta, learning rate is not required\n",
    "    'optimizer': 'adam',\n",
    "    'loss_type': 'hinge',\n",
    "    'm_loss': float(1),\n",
    "    'weight_decay': float(1e-4),\n",
    "\n",
    "    'epochs': 50,\n",
    "    'batch_size': 128,\n",
    "\n",
    "    'user_embed_size': 50,\n",
    "    'item_embed_size': 50,\n",
    "    \n",
    "    'hidden_size': 32,\n",
    "    'latent_size': 16,\n",
    "\n",
    "    'number_users_to_keep': 1000000000000,\n",
    "    'batch_log_interval': 3000,\n",
    "}\n",
    "\n",
    "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
    "if hyper_params['optimizer'] != 'adadelta':\n",
    "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
    "file_name += '_user_embed_size_' + str(hyper_params['user_embed_size'])\n",
    "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
    "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
    "\n",
    "hyper_params['log_file'] = 'saved_logs/' + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
    "hyper_params['model_file_name'] = 'saved_models/' + hyper_params['project_name'] + '_model' + file_name + '.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(hyper_params):\n",
    "    \n",
    "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
    "    \n",
    "    train = load_obj_json(hyper_params['data_base'] + 'train_ranking_vae')\n",
    "    test = load_obj_json(hyper_params['data_base'] + 'test_ranking_vae')\n",
    "    user_hist = load_obj_json(hyper_params['data_base'] + 'user_hist_ranking_vae')\n",
    "    item_hist = load_obj_json(hyper_params['data_base'] + 'item_hist_ranking_vae')\n",
    "\n",
    "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
    "\n",
    "    train_reader = DataReader(hyper_params, train, len(user_hist), item_hist, True)\n",
    "    test_reader = DataReader(hyper_params, test, len(user_hist), item_hist, False)\n",
    "\n",
    "    return train_reader, test_reader, len(user_hist), len(item_hist)\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, hyper_params, data, num_users, item_hist, is_training):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.item_hist = item_hist\n",
    "        self.num_users = num_users\n",
    "        self.num_items = len(item_hist)\n",
    "        self.data = data\n",
    "        self.is_training = is_training\n",
    "        self.all_users = []\n",
    "\n",
    "        self.number_users()\n",
    "        self.number()\n",
    "\n",
    "    def number(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "        y_batch = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                for ii in range(i+1, len(self.data[user])):\n",
    "                    if self.data[user][i][1] == self.data[user][ii][1]: continue\n",
    "\n",
    "                    y_batch.append(0)\n",
    "\n",
    "                    if len(y_batch) == self.batch_size:\n",
    "                        y_batch = []\n",
    "                        count += 1\n",
    "\n",
    "        self.num_b = count\n",
    "\n",
    "    def number_users(self):\n",
    "        users_done = 0\n",
    "        count = 0\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            y_batch = []\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                for ii in range(i+1, len(self.data[user])):\n",
    "                    if self.data[user][i][1] == self.data[user][ii][1]: continue\n",
    "\n",
    "                    y_batch.append(0)\n",
    "                    y_batch.append(0)\n",
    "\n",
    "            if len(y_batch) > 0:\n",
    "                count += 1\n",
    "                self.all_users.append(user)\n",
    "\n",
    "        self.num_u = count\n",
    "\n",
    "    def iter(self):\n",
    "        users_done = 0\n",
    "\n",
    "        x_batch_user = []\n",
    "        x_batch_item = []\n",
    "        y_batch = []\n",
    "\n",
    "        for user in self.data:\n",
    "\n",
    "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
    "            users_done += 1\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                for ii in range(i+1, len(self.data[user])):\n",
    "                    # If rating is equal, fuck off\n",
    "                    if self.data[user][i][1] == self.data[user][ii][1]: continue\n",
    "\n",
    "                    first, second = i, ii\n",
    "                    \n",
    "                    # Uncomment the below line to always send the greater one first, i.e y = 1.0 always\n",
    "                    # if self.data[user][i][1] < self.data[user][ii][1]: first, second = second, first\n",
    "\n",
    "                    x_batch_user.append(int(user))\n",
    "                    x_batch_user.append(int(user))\n",
    "\n",
    "                    x_batch_item.append(self.data[user][first][0])\n",
    "                    x_batch_item.append(self.data[user][second][0])\n",
    "                        \n",
    "                    y = 1.0\n",
    "                    if float(self.data[user][first][1]) < float(self.data[user][second][1]): y = -1.0\n",
    "                    if y < 0.0 and self.hyper_params['loss_type'] == 'bce': y = 0.0\n",
    "                    y_batch.append(float(y))\n",
    "\n",
    "                    if len(y_batch) == self.batch_size:\n",
    "\n",
    "                        yield [\n",
    "                            Variable(LongTensor(x_batch_user[::2])), \n",
    "                            Variable(LongTensor(x_batch_item[::2])),\n",
    "                        ], [\n",
    "                            Variable(LongTensor(x_batch_user[1::2])), \n",
    "                            Variable(LongTensor(x_batch_item[1::2])),\n",
    "                        ], Variable(FloatTensor(y_batch))\n",
    "                        \n",
    "                        x_batch_user = []\n",
    "                        x_batch_item = []\n",
    "                        y_batch = []\n",
    "\n",
    "    def iter_eval(self):\n",
    "\n",
    "        num_active_users = len(self.all_users)\n",
    "        num_active_users = min(num_active_users, self.hyper_params['number_users_to_keep'])\n",
    "\n",
    "        for user_now in tqdm(range(num_active_users)):\n",
    "            user = self.all_users[user_now]\n",
    "\n",
    "            x_batch_user = []\n",
    "            x_batch_item = []\n",
    "            y_batch = []\n",
    "            all_movies = []\n",
    "\n",
    "            for i in range(len(self.data[user])):\n",
    "                all_movies.append(self.data[user][i][0])\n",
    "\n",
    "                for ii in range(i+1, len(self.data[user])):\n",
    "                    # If rating is equal, fuck off\n",
    "                    if self.data[user][i][1] == self.data[user][ii][1]: continue\n",
    "\n",
    "                    first, second = i, ii\n",
    "                    \n",
    "                    # Uncomment the below line to always send the greater one first, i.e y = 1.0 always\n",
    "                    # if self.data[user][i][1] < self.data[user][ii][1]: first, second = second, first\n",
    "\n",
    "                    x_batch_user.append(int(user))\n",
    "                    x_batch_user.append(int(user))\n",
    "\n",
    "                    x_batch_item.append(self.data[user][first][0])\n",
    "                    x_batch_item.append(self.data[user][second][0])\n",
    "\n",
    "                    y_batch.append(float(self.data[user][first][1])) # Already divided by 5\n",
    "                    y_batch.append(float(self.data[user][second][1])) # Already divided by 5\n",
    "\n",
    "            if len(y_batch) > 0:\n",
    "                # print(y_batch)\n",
    "                yield [\n",
    "                    Variable(LongTensor(x_batch_user[::2])),\n",
    "                    Variable(LongTensor(x_batch_item[::2])),\n",
    "                ], [\n",
    "                    Variable(LongTensor(x_batch_user[1::2])), \n",
    "                    Variable(LongTensor(x_batch_item[1::2])),\n",
    "                ], Variable(FloatTensor(y_batch[::2])), Variable(FloatTensor(y_batch[1::2])), all_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_int(a):\n",
    "    if float(a.data) < 0.0: return -1\n",
    "    if float(a.data) > 0.0: return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_ndcg(model, criterion, reader, hyper_params):\n",
    "    model.eval()\n",
    "\n",
    "    ret = 0.0\n",
    "    \n",
    "    NDCG = {}\n",
    "    total_NDCG = {}\n",
    "\n",
    "    Ks = [1, 5, 10, 15, 20]\n",
    "\n",
    "    for k in Ks:\n",
    "        NDCG[str(k)] = 0.0\n",
    "        total_NDCG[str(k)] = 0.0\n",
    "\n",
    "    total = 0\n",
    "    user_done = 0\n",
    "\n",
    "    for x1, x2, y1, y2, all_movies in reader.iter_eval():\n",
    "        user_done += 1\n",
    "\n",
    "        _, output1 = model(x1)\n",
    "        _, output2 = model(x2)\n",
    "\n",
    "        y_diff = torch.gt(y1, y2).float() - torch.gt(y1, y2).float()\n",
    "        out_diff = torch.gt(output1, output2).float() - torch.lt(output1, output2).float()\n",
    "\n",
    "        y_pair_map = {}\n",
    "        out_pair_map = {}\n",
    "        y_r = {}\n",
    "        for i in range(x1[0].shape[0]):\n",
    "            # if float(y_diff[i].data) == 0.0: continue\n",
    "            \n",
    "            m1 = int(x1[1][i])\n",
    "            m2 = int(x2[1][i])\n",
    "\n",
    "            y_r[str(m1)] = round(float(y1[i].data), 2)\n",
    "            y_r[str(m2)] = round(float(y2[i].data), 2)\n",
    "\n",
    "            if m1 not in y_pair_map: y_pair_map[m1] = {}\n",
    "            if m2 not in y_pair_map: y_pair_map[m2] = {}\n",
    "            if m1 not in out_pair_map: out_pair_map[m1] = {}\n",
    "            if m2 not in out_pair_map: out_pair_map[m2] = {}\n",
    "\n",
    "            y_pair_map[m1][m2] = int(y_diff[i])\n",
    "            y_pair_map[m2][m1] = -1 * int(y_diff[i])\n",
    "\n",
    "            out_pair_map[m1][m2] = map_int(out_diff[i])\n",
    "            out_pair_map[m2][m1] = -1 * map_int(out_diff[i])\n",
    "\n",
    "        def compare_out(item1, item2):\n",
    "            if item2 not in out_pair_map[item1]: return 0\n",
    "            return out_pair_map[item1][item2]\n",
    "\n",
    "        all_movies = sorted(all_movies, key=functools.cmp_to_key(compare_out))\n",
    "        all_movies.reverse()\n",
    "        \n",
    "        final = []\n",
    "        final_sorted = []\n",
    "        for i in all_movies: \n",
    "            final.append([i, y_r[str(i)]])\n",
    "            final_sorted.append([i, y_r[str(i)]])\n",
    "        final_sorted = sorted(final_sorted, key=lambda x: x[1])\n",
    "        final_sorted.reverse()\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        for k in Ks:\n",
    "            if k <= len(final):\n",
    "                out = final[:k]\n",
    "                out_sorted = final_sorted[:k]\n",
    "\n",
    "                now = 0.0\n",
    "                now_best = 0.0\n",
    "                for i in range(k):\n",
    "                    now += float(out[i][1]) / float(np.log2(i+2))\n",
    "                    now_best += float(out_sorted[i][1]) / float(np.log2(i+2))\n",
    "\n",
    "                NDCG[str(k)] += float(now) / float(now_best)\n",
    "                total_NDCG[str(k)] += 1.0\n",
    "\n",
    "    for k in NDCG:\n",
    "        if total_NDCG[k] > 0: NDCG[k] = float(NDCG[k]) / float(total_NDCG[k])\n",
    "        NDCG[k] *= 100.0\n",
    "        NDCG[k] = round(NDCG[k], 4)\n",
    "\n",
    "    return NDCG\n",
    "\n",
    "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['CP'] = 0.0\n",
    "    metrics['ZEROS'] = 0.0\n",
    "    metrics['loss'] = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    batch = 0\n",
    "\n",
    "    for x1, x2, y in reader.iter():\n",
    "        batch += 1\n",
    "        if is_train_set == True and batch > hyper_params['testing_batch_limit']: break\n",
    "\n",
    "        o1, output1 = model(x1)\n",
    "        o2, output2 = model(x2)\n",
    "        out_diff = torch.gt(output1, output2).float() - torch.lt(output1, output2).float()\n",
    "\n",
    "        metrics['loss'] += criterion(o1 + o2, [output1] + [output2], y, x1[1], x2[1]).data\n",
    "        \n",
    "        temp_correct  = int(torch.sum(torch.lt(y, 0.0) * torch.lt(out_diff, 0.0)))\n",
    "        temp_correct += int(torch.sum(torch.gt(y, 0.0) * torch.gt(out_diff, 0.0)))\n",
    "\n",
    "        temp_not_correct  = int(torch.sum(torch.lt(y, 0.0) * torch.gt(out_diff, 0.0)))\n",
    "        temp_not_correct += int(torch.sum(torch.gt(y, 0.0) * torch.lt(out_diff, 0.0)))\n",
    "\n",
    "        temp_zeros = int(torch.sum(torch.eq(out_diff, 0.0)))\n",
    "\n",
    "        correct += temp_correct\n",
    "        not_correct += temp_not_correct\n",
    "        zeros += temp_zeros\n",
    "        total += int(y.shape[0])\n",
    "\n",
    "        assert temp_correct + temp_not_correct + temp_zeros == int(y.shape[0])\n",
    "\n",
    "    assert correct + not_correct + zeros == total\n",
    "\n",
    "    metrics['CP'] = float(correct) / float(total)\n",
    "    metrics['CP'] *= 100.0\n",
    "    metrics['CP'] = round(metrics['CP'], 4)\n",
    "\n",
    "    metrics['ZEROS'] = float(zeros) / float(total)\n",
    "    metrics['ZEROS'] *= 100.0\n",
    "    metrics['ZEROS'] = round(metrics['ZEROS'], 4)\n",
    "\n",
    "    metrics['loss'] = float(metrics['loss'][0]) / float(batch)\n",
    "    metrics['loss'] = round(metrics['loss'], 4)\n",
    "\n",
    "    if is_train_set == False:\n",
    "        ndcg = evaluate_ndcg(model, criterion, reader, hyper_params)\n",
    "        for k in ndcg: metrics['NDCG@' + str(k)] = ndcg[k]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f895fe2f529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         self.linear1 = nn.Linear(\n\u001b[1;32m      5\u001b[0m             \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_embed_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_embed_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            hyper_params['user_embed_size'] + hyper_params['item_embed_size'], hyper_params['hidden_size']\n",
    "        )\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
    "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
    "        nn.init.xavier_normal(self.linear1.weight)\n",
    "        nn.init.xavier_normal(self.linear2.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Model, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.encoder = Encoder(hyper_params)\n",
    "        self.decoder = Decoder(hyper_params)\n",
    "        \n",
    "        self._enc_mu = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        self._enc_log_sigma = nn.Linear(hyper_params['hidden_size'], hyper_params['latent_size'])\n",
    "        nn.init.xavier_normal(self._enc_mu.weight)\n",
    "        nn.init.xavier_normal(self._enc_log_sigma.weight)\n",
    "        \n",
    "        self.user_embed = nn.Embedding(hyper_params['total_users'], hyper_params['user_embed_size'])\n",
    "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
    "        nn.init.normal(self.user_embed.weight.data, mean=0, std=0.01)\n",
    "        nn.init.normal(self.item_embed.weight.data, mean=0, std=0.01)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation_last = nn.Tanh()\n",
    "        if self.hyper_params['loss_type'] == 'bce': self.activation_last = nn.Sigmoid()\n",
    "        \n",
    "        prev = hyper_params['latent_size']\n",
    "        self.layer_hinge1 = nn.Linear(prev, 1)\n",
    "#         self.layer_hinge2 = nn.Linear(64, 1)\n",
    "        nn.init.xavier_normal(self.layer_hinge1.weight)\n",
    "#         nn.init.xavier_normal(self.layer_hinge2.weight)\n",
    "        # xavier_uniform\n",
    "        \n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float().cuda()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        user = self.user_embed(x[0])\n",
    "        item = self.item_embed(x[1])\n",
    "        \n",
    "        h_enc = self.encoder(torch.cat([user, item], dim=-1))\n",
    "        z = self.sample_latent(h_enc)\n",
    "        dec = self.decoder(z)\n",
    "              \n",
    "        output = self.layer_hinge1(z)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.layer_hinge2(output)\n",
    "        output = self.activation_last(output)\n",
    "                              \n",
    "        return [\n",
    "            dec, self.z_mean, self.z_sigma\n",
    "        ], output.squeeze(-1)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                hyper_params['user_embed'] + hyper_params['item_embed'] + 1, 64\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        validity = self.model(z)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(VAELoss,self).__init__()\n",
    "\n",
    "        self.loss_type = hyper_params['loss_type']\n",
    "        self.m_loss = hyper_params['m_loss']\n",
    "        batch_size = hyper_params['batch_size']\n",
    "\n",
    "        self.zeros_while_max = torch.zeros(int(batch_size)).float()\n",
    "        if is_cuda_available: self.zeros_while_max = self.zeros_while_max.cuda()\n",
    "        self.zeros_while_max = Variable(self.zeros_while_max)\n",
    "        self.exp = Variable(FloatTensor([np.e]))\n",
    "        self.hundred = Variable(FloatTensor([100.0]))\n",
    "        self.cce = nn.CrossEntropyLoss(size_average=True)\n",
    "        self.bce = nn.BCELoss(size_average=True)\n",
    "\n",
    "    def forward(self, o1, o2, y, t1, t2):\n",
    "        \n",
    "        m1, zm1, zs1, m2, zm2, zs2 = o1\n",
    "        \n",
    "        mean_sq1 = zm1 * zm1\n",
    "        stddev_sq1 = zs1 * zs1\n",
    "        kld  = torch.mean(mean_sq1 + stddev_sq1 - torch.log(stddev_sq1) - 1)\n",
    "        \n",
    "        mean_sq2 = zm2 * zm2\n",
    "        stddev_sq2 = zs2 * zs2\n",
    "        kld += torch.mean(mean_sq2 + stddev_sq2 - torch.log(stddev_sq2) - 1)\n",
    "        \n",
    "        likelihood  = self.cce(m1, t1) # Try using a weight parameters, alpha when summing\n",
    "        likelihood += self.cce(m2, t2) # Try using a weight parameters, alpha when summing\n",
    "        \n",
    "#         out_diff = torch.gt(o2[0], o2[1]).float() - torch.lt(o2[0], o2[1]).float()\n",
    "        out_diff = o2[0] - o2[1]\n",
    "        \n",
    "        # Reference: https://papers.nips.cc/paper/3708-ranking-measures-and-loss-functions-in-learning-to-rank.pdf\n",
    "        if self.loss_type == 'hinge':\n",
    "            pairwise_loss = self.m_loss - (y * out_diff)\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "            \n",
    "        elif self.loss_type == 'bce':\n",
    "            pairwise_loss = self.bce(out_diff, y)\n",
    "            \n",
    "        elif self.loss_type == 'easy_hinge':\n",
    "            # pairwise_loss = torch.log(2.0 - (y * o2))# / np.log(2) # torch.log is base \"e\"\n",
    "            pairwise_loss = torch.log((out_diff*out_diff) - (10*y*out_diff) + 26) - 2\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "            \n",
    "        elif self.loss_type == 'difficult_hinge':\n",
    "            # pairwise_loss = torch.log(2.0 - (y * o2))# / np.log(2) # torch.log is base \"e\"\n",
    "            pairwise_loss = torch.pow(self.hundred, 1 - (y * out_diff)) - 1\n",
    "            pairwise_loss = torch.mean(torch.max(self.zeros_while_max, pairwise_loss))\n",
    "        \n",
    "        elif self.loss_type == 'saddle':\n",
    "            pairwise_loss = torch.pow(y + out_diff, 2)\n",
    "\n",
    "        elif self.loss_type == 'exp':\n",
    "            pairwise_loss = torch.pow(self.exp, y * out_diff)\n",
    "\n",
    "        elif self.loss_type == 'logistic':\n",
    "            pairwise_loss = torch.log(self.m_loss + torch.pow(self.exp, -(y * out_diff)))\n",
    "        \n",
    "        final = (0.05 * kld) + (0.2 * likelihood) + (0.75 * pairwise_loss)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading data file\n",
      "Data Files loaded!\n",
      "\n",
      "\n",
      "Simulation run on: 2018-07-06 17:27:28.539250\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches: 15574\n",
      "Number of test batches:  953\n",
      "Total Users: 3131\n",
      "Total Items: 3274\n",
      "\n",
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (linear1): Linear(in_features=100, out_features=32, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (linear2): Linear(in_features=32, out_features=3274, bias=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (_enc_log_sigma): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (user_embed): Embedding(3131, 50)\n",
      "  (item_embed): Embedding(3274, 50)\n",
      "  (activation): ReLU()\n",
      "  (activation_last): Tanh()\n",
      "  (layer_hinge1): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n",
      "| epoch   1 |  3000/15574 batches | ms/batch  5.39 | loss 3.5258\n",
      "| epoch   1 |  6000/15574 batches | ms/batch  5.27 | loss 2.9975\n",
      "| epoch   1 |  9000/15574 batches | ms/batch  5.27 | loss 2.4919\n",
      "| epoch   1 | 12000/15574 batches | ms/batch  5.29 | loss 2.1519\n",
      "| epoch   1 | 15000/15574 batches | ms/batch  5.66 | loss 1.7931\n",
      "| epoch   1 | 15574/15574 batches | ms/batch  5.31 | loss 1.5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3087/3087 [00:29<00:00, 104.38it/s]\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/serverfujitsu/noveen/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 118.45s | CP = 66.2013 | ZEROS = 0.0 | loss = 1.7359 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 118.45s | CP = 61.595 | ZEROS = 0.0 | loss = 2.0527 | NDCG@1 = 83.518 | NDCG@5 = 88.6499 | NDCG@10 = 90.3006 | NDCG@15 = 92.2734 | NDCG@20 = 95.3033 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  3000/15574 batches | ms/batch  5.25 | loss 1.5415\n",
      "| epoch   2 |  6000/15574 batches | ms/batch  5.40 | loss 1.4159\n",
      "| epoch   2 |  9000/15574 batches | ms/batch  5.33 | loss 1.2980\n",
      "| epoch   2 | 12000/15574 batches | ms/batch  5.37 | loss 1.2865\n",
      "| epoch   2 | 15000/15574 batches | ms/batch  5.21 | loss 1.1982\n",
      "| epoch   2 | 15574/15574 batches | ms/batch  5.63 | loss 1.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3087/3087 [00:29<00:00, 103.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 117.88s | CP = 67.3113 | ZEROS = 0.0 | loss = 1.2939 (TRAIN)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 117.88s | CP = 62.4705 | ZEROS = 0.0 | loss = 1.5404 | NDCG@1 = 84.2744 | NDCG@5 = 88.8561 | NDCG@10 = 90.4742 | NDCG@15 = 92.485 | NDCG@20 = 95.4997 (TEST)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  3000/15574 batches | ms/batch  4.56 | loss 1.1601\n"
     ]
    }
   ],
   "source": [
    "def train(reader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    batch = 0\n",
    "    batch_limit = int(train_reader.num_b)\n",
    "\n",
    "    for x1, x2, y in reader.iter():\n",
    "        batch += 1\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        temp_o1, temp_o2 = model(x1)\n",
    "        temp_o3, temp_o4 = model(x2)\n",
    "        \n",
    "        loss = criterion(temp_o1 + temp_o3, [temp_o2] + [temp_o4], y, x1[1], x2[1])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
    "            div = hyper_params['batch_log_interval']\n",
    "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
    "            if div <= 0: div = 1\n",
    "\n",
    "            cur_loss = (total_loss[0] / div)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
    "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
    "            )\n",
    "            \n",
    "            file_write(hyper_params['log_file'], ss)\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "train_reader, test_reader, total_users, total_items = load_data(hyper_params)\n",
    "hyper_params['total_users'] = total_users\n",
    "hyper_params['total_items'] = total_items\n",
    "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
    "\n",
    "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
    "file_write(hyper_params['log_file'], \"Total Users: \" + str(total_users))\n",
    "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
    "\n",
    "model = Model(hyper_params)\n",
    "if is_cuda_available: model.cuda()\n",
    "\n",
    "criterion = VAELoss(hyper_params)\n",
    "\n",
    "if hyper_params['optimizer'] == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "elif hyper_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), weight_decay=hyper_params['weight_decay']#, lr=hyper_params['learning_rate']\n",
    "    )\n",
    "\n",
    "file_write(hyper_params['log_file'], str(model))\n",
    "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(train_reader)\n",
    "        \n",
    "        # Calulating the metrics on the train set\n",
    "        metrics = evaluate(model, criterion, train_reader, hyper_params, True)\n",
    "        string = \"\"\n",
    "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string += ' (TRAIN)'\n",
    "    \n",
    "        # Calulating the metrics on the test set\n",
    "        metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "        string2 = \"\"\n",
    "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "        string2 += ' (TEST)'\n",
    "\n",
    "        ss  = '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
    "        ss += string2\n",
    "        ss += '\\n'\n",
    "        ss += '-' * 89\n",
    "        file_write(hyper_params['log_file'], ss)\n",
    "        \n",
    "        if not best_val_loss or metrics['loss'] <= best_val_loss:\n",
    "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
    "            best_val_loss = metrics['loss']\n",
    "\n",
    "except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
    "metrics = evaluate(model, criterion, test_reader, hyper_params, False)\n",
    "\n",
    "string = \"\"\n",
    "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
    "\n",
    "ss  = '=' * 89\n",
    "ss += '\\n| End of training'\n",
    "ss += string\n",
    "ss += '\\n'\n",
    "ss += '=' * 89\n",
    "file_write(hyper_params['log_file'], ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "223px",
    "width": "193px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Notebook contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
